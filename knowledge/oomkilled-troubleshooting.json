{
  "id": "oomkilled-troubleshooting",
  "title": "OOMKilled and Memory Pressure",
  "category": "troubleshooting",
  "tags": [
    "oom",
    "oomkilled",
    "memory",
    "exit137",
    "limit",
    "resources",
    "eviction"
  ],
  "summary": "Resolve OOMKilled (exit 137) by checking limits/requests, actual usage, and node pressure; includes JVM guidance and profiling hints.",
  "error_patterns": [
    "OOMKilled",
    "exit code 137",
    "Killed",
    "memory.*exceeded",
    "Out of memory",
    "cannot allocate memory"
  ],
  "recommended_tools": [
    "DESCRIBE Pod <ns>/<pod>",
    "TOP_PODS",
    "GET_EVENTS <ns>",
    "FIND_ISSUES"
  ],
  "quick_fix": "kubectl describe pod -n <ns> <pod> | grep -i OOMKilled && kubectl top pod -n <ns> <pod> — raise limits/requests or reduce usage.",
  "symptoms": [
    "Pod status shows OOMKilled",
    "Exit code 137",
    "Restarts climb while memory rises",
    "Node memory pressure/evictions"
  ],
  "decision_tree": [
    "If limits too low vs usage → raise memory request/limit or optimize app.",
    "If JVM: set -Xmx to ~70–75% of limit; avoid container-invisible defaults.",
    "If no limits and node under pressure → set limits/requests to avoid eviction; add capacity.",
    "If memory leak → profile and fix; add liveness only after fixing leak (not as band-aid)."
  ],
  "common_causes": [
    {
      "cause": "Memory limit too low",
      "description": "App uses more than set limit.",
      "diagnosis": "Compare kubectl top pod vs spec limits.",
      "fix": "Raise memory limits/requests to match real usage."
    },
    {
      "cause": "Memory leak",
      "description": "Usage climbs steadily until killed.",
      "diagnosis": "Watch kubectl top pod --watch; inspect app metrics.",
      "fix": "Profile and fix leak; restart after fix; consider HPA based on memory."
    },
    {
      "cause": "JVM heap misconfig",
      "description": "JVM heap exceeds container limit.",
      "diagnosis": "Check -Xmx/-XX:MaxRAMPercentage vs limit.",
      "fix": "Set -Xmx to ~70–75% of container memory; tune MaxRAMPercentage."
    },
    {
      "cause": "Node pressure/eviction",
      "description": "No pod limits; node evicts under pressure.",
      "diagnosis": "Events show eviction; no limits in pod spec.",
      "fix": "Set requests/limits; add capacity or reduce node load."
    }
  ],
  "diagnostic_commands": [
    "kubectl describe pod -n <ns> <pod> | grep -i OOMKilled -A3",
    "kubectl top pod -n <ns> <pod>",
    "kubectl top nodes",
    "kubectl get pod -n <ns> <pod> -o jsonpath='{.spec.containers[*].resources}'"
  ],
  "investigation_flow": [
    "1) Confirm OOM: describe pod, check lastState.terminated.reason=OOMKilled/exit 137.",
    "2) Check usage vs limits: kubectl top pod -n <ns> <pod>; compare to spec.",
    "3) If usage ~ limit → raise limits/requests OR reduce footprint; for JVM, set -Xmx ~70–75% of limit.",
    "4) If leak: monitor top over time; profile app; fix leak.",
    "5) If node pressure: set limits/requests; add capacity; spread workloads.",
    "6) Validate: restart pod or rollout; ensure restarts stop and memory stabilizes."
  ]
}
