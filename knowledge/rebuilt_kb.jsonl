{"id": "cluster-api-troubleshooting", "category": "troubleshooting", "symptoms": ["Machines not provisioning", "Clusters not becoming Ready", "Errors in bootstrap/infrastructure", "Failed to create", "Failed to reconcile", "bootstrap failed", "infrastructure cluster not ready", "Controller health - capi-controller-manager or provider controllers not running.", "Provider auth/quota - Cloud auth/quota issues for infra machines.", "Bootstrap failure - Cloud-init/bootstrap script fails."], "root_cause": "Debug Cluster API provisioning: controller health, Machine/MachineDeployment status, infra errors, and credentials.", "investigation": ["kubectl get clusters -A", "kubectl get machines -A", "kubectl describe machine -n <ns> <name>", "kubectl logs -n capi-system deploy/capi-controller-manager | tail -n 100", "kubectl logs -n <provider-ns> deploy/*provider* | tail -n 100", "1) Check CAPI controllers and provider controllers health.", "2) Inspect Machine/MachineDeployment conditions for errors.", "3) Review controller logs for auth/quota/bootstrap errors.", "4) Fix credentials/quota/bootstrap config; trigger reconcile.", "5) Confirm Machines become Ready and Cluster is Ready.", "kubectl get pods -n capi-system; provider namespaces.", "Logs/events show auth/quota errors.", "Check bootstrap logs on machine; conditions show bootstrap errors."], "fixes": ["Check CAPI controllers, Machine/MachineDeployment conditions, and infra provider logs; fix credentials/infra errors and retry reconcile.", "Restart/fix controller deployments.", "Update credentials; increase quotas.", "Fix bootstrap data; ensure secrets/templates correct."], "related_patterns": ["capi", "cluster-api", "provisioning", "machines", "infrastructure", "Failed to create", "Failed to reconcile", "bootstrap failed", "infrastructure cluster not ready"]}
{"id": "service-connectivity-troubleshooting", "category": "troubleshooting", "symptoms": ["HTTP 503/504 or connection refused", "Service has no endpoints", "Pods are Running but not Ready", "Traffic works from some pods/namespaces but not others", "503", "504", "connection refused", "no endpoints", "endpoint .* not found", "service unavailable", "connection timed out", "connection reset", "Selector mismatch - Service selector labels do not match pods.", "Pods not Ready - Readiness probe failing or app not healthy.", "Port/targetPort mismatch - Service targetPort does not match container port.", "NetworkPolicy blocking - Policy denies traffic from client to pod.", "No backing pods - Deployment scaled to zero or pods CrashLooping/NotReady."], "root_cause": "Actionable playbook for Service 503/504/connection refused/no endpoints: endpoints, selectors, readiness, ports, and NetworkPolicy.", "investigation": ["kubectl get endpoints -n <ns> <svc> -o wide", "kubectl describe svc -n <ns> <svc>", "kubectl get pod -n <ns> -l <selector> -o wide", "kubectl describe pod -n <ns> <pod> | sed -n '/Readiness/Liveness/,$p'", "kubectl get events -n <ns> --sort-by=.lastTimestamp | tail -n 30", "kubectl get networkpolicy -n <ns> -o yaml", "1) Discover service/pods: LIST_ALL Service/Pod in <ns>; get <svc> and backing pods.", "2) Endpoints: GET_ENDPOINTS <ns> <svc>. If none, fix selector or scale pods.", "3) Selectors: DESCRIBE Service <svc>; compare selector to pod labels (LIST_ALL Pod).", "4) Readiness: DESCRIBE Pod <pod> for readiness probe failures; fix path/port/timing; ensure app listens.", "5) Ports: Verify Service port/targetPort matches containerPorts.", "6) NetworkPolicy: If endpoints ready but 503/timeout, check policies; allow traffic from clients.", "7) Confirm: retest traffic; endpoints should be Ready; 503/timeout resolved.", "DESCRIBE Service selector vs pod labels; GET_ENDPOINTS empty.", "Pods show Ready 0/1; describe pod shows readiness probe failures.", "DESCRIBE Service vs containerPorts; endpoints list unexpected port.", "Traffic works from some namespaces only; inspect NetworkPolicies in ns.", "LIST_ALL Pod shows 0 or CrashLoopBackOff."], "fixes": ["GET_ENDPOINTS <ns> <svc>; if empty, fix Service selector vs pod labels and ensure pods are Ready. If endpoints exist but 503/timeout, check readiness probes and NetworkPolicy.", "Align Service selectors with pod labels or relabel pods.", "Fix readiness endpoint/port; relax probe timing; ensure app listens.", "Set targetPort to actual container port.", "Allow ingress from client namespaces/pods to pod ports; test without policy.", "Scale up deployment; fix pod health."], "related_patterns": ["service", "endpoint", "503", "504", "connection refused", "timeout", "network", "no endpoints", "endpoint .* not found", "service unavailable", "connection timed out", "connection reset"]}
{"id": "cronjob-job-troubleshooting", "category": "troubleshooting", "symptoms": ["Jobs fail repeatedly (BackoffLimitExceeded)", "Jobs time out (DeadlineExceeded)", "CronJob not creating Jobs", "CronJob creates overlapping Jobs", "Old Jobs/Pods not cleaned up", "BackoffLimitExceeded", "DeadlineExceeded", "no Jobs created", "concurrencyPolicy", "suspend: true", "Application/config error - Job pods exit non-zero repeatedly.", "Schedule issues - Cron expression invalid or missed deadlines; suspend true.", "Overlapping runs - Jobs pile up when previous hasn’t finished.", "No cleanup - Old Jobs/Pods consume resources."], "root_cause": "Fix batch workloads that fail, don’t run, or pile up: backoff limits, deadlines, schedules, and cleanup.", "investigation": ["kubectl describe job -n <ns> <name>", "kubectl describe cronjob -n <ns> <name>", "kubectl get pods -l job-name=<job> -n <ns>", "kubectl logs job/<name> -n <ns>", "kubectl get jobs -n <ns> | wc -l", "1) Describe Job/CronJob to see conditions/events (BackoffLimitExceeded/DeadlineExceeded).", "2) Get pod logs for the job: kubectl logs job/<name> -n <ns> (--previous if retries).", "3) If BackoffLimitExceeded → fix app/config; adjust backoffLimit if needed.", "4) If DeadlineExceeded → raise activeDeadlineSeconds or optimize job.", "5) If CronJob not running → validate cron; ensure suspend: false; startingDeadlineSeconds reasonable.", "6) If overlapping → set concurrencyPolicy: Forbid/Replace; adjust schedule.", "7) Add history limits/ttlSecondsAfterFinished to clean old Jobs.", "kubectl logs job/<name> -n <ns>; describe job for conditions.", "Describe cronjob; check suspend/startingDeadlineSeconds; validate cron at crontab.guru.", "Multiple Jobs with same cronjob-name label.", "Many completed/failed Jobs present."], "fixes": ["Describe the Job/CronJob to see failures; check pod logs; adjust backoffLimit/activeDeadlineSeconds; validate schedule/concurrencyPolicy.", "Fix app/config; optionally increase backoffLimit if transient.", "Correct schedule; set suspend: false; adjust startingDeadlineSeconds.", "Set concurrencyPolicy: Forbid/Replace; increase interval; speed up job.", "Set successfulJobsHistoryLimit/failedJobsHistoryLimit; use ttlSecondsAfterFinished."], "related_patterns": ["cronjob", "job", "batch", "backoff", "deadline", "BackoffLimitExceeded", "DeadlineExceeded", "no Jobs created", "concurrencyPolicy", "suspend: true"]}
{"id": "hpa-scaling-troubleshooting", "category": "troubleshooting", "symptoms": ["HPA not scaling up/down despite load", "Events show missing metrics", "Replicas stuck at min or max", "failed to get cpu utilization", "metrics not available", "unable to compute replicas", "did not receive metrics", "Metrics server not working - No resource metrics available for HPA.", "Targets misconfigured - CPU/memory targets unreachable or too lax.", "Pods not Ready or throttled - Unready pods excluded from metrics; throttling affects metrics."], "root_cause": "Fix HPAs that won't scale up/down: verify metrics availability, targets, min/max, and pod readiness.", "investigation": ["kubectl get hpa -n <ns>", "kubectl describe hpa -n <ns> <name>", "kubectl get --raw \"/apis/metrics.k8s.io/v1beta1\" | head", "kubectl top pods -n <ns>", "kubectl get events -n <ns> --sort-by=.lastTimestamp | tail -n 30", "1) Describe HPA to see current/target metrics and events.", "2) Verify metrics-server/API: kubectl get --raw /apis/metrics.k8s.io/v1beta1.", "3) Check pod readiness and resource usage (kubectl top pods).", "4) Adjust target utilization and min/max replicas if needed.", "5) If at max and still high load, increase max or optimize workload.", "6) Confirm scaling by applying load test; watch HPA status.", "Describe HPA shows missing metrics; metrics.k8s.io unavailable.", "Describe HPA shows current vs target utilization.", "Check pod readiness; TOP_PODS for CPU/mem throttling."], "fixes": ["Describe the HPA to see current metrics/targets; ensure metrics-server is running; adjust target thresholds and min/max replicas.", "Install/fix metrics-server; ensure RBAC/certs correct.", "Set realistic target utilization; adjust min/max replicas.", "Fix readiness; adjust limits/requests."], "related_patterns": ["hpa", "autoscaling", "cpu", "memory", "metrics", "scaling", "failed to get cpu utilization", "metrics not available", "unable to compute replicas", "did not receive metrics"]}
{"id": "oomkilled-troubleshooting", "category": "troubleshooting", "symptoms": ["Pod status shows OOMKilled", "Exit code 137", "Restarts climb while memory rises", "Node memory pressure/evictions", "OOMKilled", "exit code 137", "Killed", "memory.*exceeded", "Out of memory", "cannot allocate memory", "Memory limit too low - App uses more than set limit.", "Memory leak - Usage climbs steadily until killed.", "JVM heap misconfig - JVM heap exceeds container limit.", "Node pressure/eviction - No pod limits; node evicts under pressure."], "root_cause": "Resolve OOMKilled (exit 137) by checking limits/requests, actual usage, and node pressure; includes JVM guidance and profiling hints.", "investigation": ["kubectl describe pod -n <ns> <pod> | grep -i OOMKilled -A3", "kubectl top pod -n <ns> <pod>", "kubectl top nodes", "kubectl get pod -n <ns> <pod> -o jsonpath='{.spec.containers[*].resources}'", "1) Confirm OOM: describe pod, check lastState.terminated.reason=OOMKilled/exit 137.", "2) Check usage vs limits: kubectl top pod -n <ns> <pod>; compare to spec.", "3) If usage ~ limit → raise limits/requests OR reduce footprint; for JVM, set -Xmx ~70–75% of limit.", "4) If leak: monitor top over time; profile app; fix leak.", "5) If node pressure: set limits/requests; add capacity; spread workloads.", "6) Validate: restart pod or rollout; ensure restarts stop and memory stabilizes.", "Compare kubectl top pod vs spec limits.", "Watch kubectl top pod --watch; inspect app metrics.", "Check -Xmx/-XX:MaxRAMPercentage vs limit.", "Events show eviction; no limits in pod spec."], "fixes": ["kubectl describe pod -n <ns> <pod> | grep -i OOMKilled && kubectl top pod -n <ns> <pod> — raise limits/requests or reduce usage.", "Raise memory limits/requests to match real usage.", "Profile and fix leak; restart after fix; consider HPA based on memory.", "Set -Xmx to ~70–75% of container memory; tune MaxRAMPercentage.", "Set requests/limits; add capacity or reduce node load."], "related_patterns": ["oom", "oomkilled", "memory", "exit137", "limit", "resources", "eviction", "OOMKilled", "exit code 137", "Killed", "memory.*exceeded", "Out of memory", "cannot allocate memory"]}
{"id": "crossplane-azure-providers-troubleshooting", "category": "troubleshooting", "symptoms": ["Provider not healthy - Crossplane Azure provider pod is not running", "ProviderConfig credentials issue - Azure credentials not configured or expired", "Managed resource stuck - Azure resource not syncing (conditions show errors)", "Azure API errors - Azure returning 400/403/404 errors", "Cross-resource references - Resource waiting on dependencies (VNet, subnet, etc.)"], "root_cause": "Troubleshooting Crossplane Azure providers (azure.upbound.io) for managing Azure resources", "investigation": ["kubectl get providers.pkg.crossplane.io", "kubectl get providerconfigs.azure.upbound.io", "kubectl get managed", "kubectl describe <resource-type> <name>", "kubectl get accounts.storage.azure.upbound.io", "kubectl get vaults.keyvault.azure.upbound.io", "kubectl get kubernetesclusters.containerservice.azure.upbound.io", "kubectl get providers.pkg.crossplane.io, look for HEALTHY=False", "kubectl get providerconfig, kubectl describe providerconfig", "Check managed resource conditions for Azure error messages", "Check status.conditions for dependency errors"], "fixes": ["1. Check provider health: kubectl get providers.pkg.crossplane.io", "2. Verify ProviderConfig exists with valid credentials", "3. Describe failing managed resource for conditions", "4. Check Azure portal for actual resource state", "5. Review provider pod logs for Azure API errors", "kubectl describe provider <name>, check operator logs", "Update Azure credentials secret, verify service principal or managed identity", "Check Azure portal for resource state, verify permissions", "Verify Azure subscription quota, permissions, resource naming", "Ensure referenced resources exist and are Ready"], "related_patterns": ["crossplane", "azure", "upbound", "provider", "storage", "keyvault", "network", "compute", "containerservice", "aks", "managed", "provider-family-azure", "provider-azure-storage", "provider-azure-keyvault", "provider-azure-network", "provider-azure-compute", "provider-azure-containerservice", "provider-azure-sql", "provider-azure-cosmosdb", "provider-azure-eventhub", "provider-azure-cache", "provider-azure-managedidentity", "provider-azure-authorization"]}
{"id": "uipath-automation-suite-troubleshooting", "category": "troubleshooting", "symptoms": ["Orchestrator pods not ready", "Robot connectivity issues", "Action Center not responding", "AI Center training failures", "Certificate or auth errors", "Database connection issues", "uipathctl commands failing", "Database connectivity - Cannot connect to SQL Server or PostgreSQL", "Identity server issues - Authentication failing, cannot login", "Certificate problems - TLS certs expired or misconfigured", "License issues - License expired or not activated", "Resource exhaustion - Pods OOMKilled or CPU throttled", "Storage issues - PVC not bound or storage full"], "root_cause": "Troubleshooting UiPath Automation Suite components running on managed Kubernetes (EKS/AKS)", "investigation": ["kubectl get pods -n uipath", "kubectl logs -n uipath <pod-name>", "kubectl describe pod -n uipath <pod-name>", "kubectl get certificates -n uipath", "kubectl get pvc -n uipath", "kubectl get ingress -n uipath", "kubectl top pods -n uipath", "Use uipathctl: Primary CLI tool for installation, upgrade, and troubleshooting", "Use health_check: uipathctl health check - runs cluster health validation", "Use prereq_check: uipathctl prereq check - validates prerequisites", "Use manifest_apply: uipathctl manifest apply - applies configuration", "Check orchestrator logs for SQL connection errors", "Check identity-service pods, verify OIDC config", "kubectl get certificates, check cert-manager", "Check orchestrator logs for license errors", "kubectl top pods, check resource limits", "kubectl get pvc, check storage usage"], "fixes": ["1. Check all pods in UiPath namespace are running", "2. Verify database connectivity from pods", "3. Check certificates are valid and not expired", "4. Review ingress configuration for external access", "5. Check resource usage (CPU/memory/storage)", "6. Review application logs for specific errors", "Verify connection string secret, network access, credentials", "Restart identity pods, check certificate validity", "Renew certificates, verify issuer is working", "Update license, verify license server connectivity", "Increase resource limits for affected components", "Expand PVC or clean up old data"], "related_patterns": ["uipath", "automation", "suite", "orchestrator", "robot", "rpa", "action", "center", "insights", "aicenter", "eks", "aks", "uipathctl", "identity-server", "action-center", "automation-ops", "data-service", "document-understanding", "integration-service", "automation-suite", "uipath-infra"]}
{"id": "resource-provisioning", "category": "troubleshooting", "symptoms": ["Managed resources not Ready", "Provisioning stuck in Creating/Deleting", "Cloud resources not created/updated", "Reconcile error", "failed to create", "failed to update", "cannot authenticate", "rate limit", "quota exceeded", "Controller not running/healthy - Provider/controller pods crash or missing.", "Credentials/auth failure - Cloud creds invalid/expired.", "Quota/rate limiting - Cloud API rejects due to quota/limits.", "Spec/CRD mismatch - Missing/old CRDs or incorrect fields."], "root_cause": "Debug managed resource provisioning (Crossplane/CRDs): controller health, events, cloud API errors, and credentials.", "investigation": ["kubectl get providers.pkg.crossplane.io", "kubectl get managed -A", "kubectl describe <managed-kind> -n <ns> <name>", "kubectl logs -n crossplane-system deploy/<provider> | tail -n 100", "kubectl get events -A --sort-by=.lastTimestamp | tail -n 50", "1) Check controller/provider health (pods/events).", "2) Describe managed resource for conditions; look for auth/quota/spec errors.", "3) Verify credentials/secrets and cloud permissions.", "4) If quota/rate limit → wait/back off; request quota increase.", "5) Fix spec/CRD mismatches; ensure provider versions align.", "6) Reconcile/retry; confirm managed resource Ready and cloud resource created.", "GET_CROSSPLANE; check provider pods.", "Events/status show auth errors; cloud API denied.", "Events/status mention quota/rate limit.", "kubectl describe managed resource; check conditions."], "fixes": ["Check managed resource status/conditions; inspect controller logs and events; verify cloud credentials and quotas; retry after fixing.", "Restart/fix provider deployments; ensure correct version and creds.", "Refresh credentials/secrets; validate permissions.", "Request quota increase; reduce reconcile rate.", "Install correct CRDs/providers; fix spec per API."], "related_patterns": ["crossplane", "provisioning", "controllers", "cloud", "crd", "reconcile", "Reconcile error", "failed to create", "failed to update", "cannot authenticate", "rate limit", "quota exceeded"]}
{"id": "crashloop-troubleshooting", "category": "troubleshooting", "symptoms": ["Pod status shows CrashLoopBackOff", "Container restarts multiple times (restarts > 3)", "Backoff delay increasing between restarts", "App never becomes Ready", "CrashLoopBackOff", "Back-off restarting failed container", "container .* exited with code", "Liveness probe failed", "Readiness probe failed", "OOMKilled", "Error: container .* not found", "Application error on startup - App crashes due to bug or missing dependency/config.", "OOMKilled (memory pressure) - Container killed for exceeding memory limit.", "Bad command/entrypoint - Entrypoint/command not found or not executable.", "Probe failures - Liveness/readiness probe fails; kubelet restarts container.", "Missing configuration or mounts - ConfigMap/Secret/PVC missing or wrong key/path.", "Segfault - App segfaults (SIGSEGV)."], "root_cause": "Step-by-step playbook to root-cause pods that keep restarting, with clear commands for logs, events, exit codes, probes, and resource pressure.", "investigation": ["kubectl get pod -n <ns> <pod> -o wide", "kubectl logs -n <ns> <pod> --previous | head -n 120", "kubectl describe pod -n <ns> <pod>", "kubectl get pod -n <ns> <pod> -o jsonpath='{.status.containerStatuses[*].lastState}'", "kubectl get events -n <ns> --sort-by=.lastTimestamp | tail -n 20", "kubectl top pods -A | head -n 30", "1) Discover target: FIND_ISSUES or LIST_ALL Pod to get <ns>/<pod>.", "2) Logs (previous crash): kubectl logs -n <ns> <pod> --previous | head -n 120.", "3) Events/exit codes: kubectl describe pod -n <ns> <pod> | sed -n '/Events/,$p'.", "4) Identify exit code/reason (1=app, 137=OOM, 127=cmd not found, probe failures).", "5) If OOM: kubectl top pods -A; check limits/requests and increase or optimize.", "6) If probe failing: fix probe path/port/timing; add startupProbe for slow start.", "7) If config/mount issues: verify ConfigMap/Secret/PVC names/keys; fix volumeMounts.", "8) Confirm fix: delete/restart pod or rollout restart; ensure Ready and restarts stable.", "kubectl logs -n <ns> <pod> --previous | head -n 80", "kubectl describe pod -n <ns> <pod> | grep -i OOMKilled; check TOP_PODS.", "Check logs for 'command not found'; inspect container image entrypoint/args.", "Events show 'Liveness/Readiness probe failed'; logs may be clean.", "Logs show 'file not found'/'permission denied'; describe pod shows mount errors.", "Exit code 139 in Events/lastState; logs show segfault."], "fixes": ["kubectl logs -n <ns> <pod> --previous && kubectl describe pod -n <ns> <pod> | sed -n '/Events/,$p'", "Fix the app/config, rebuild image if needed; redeploy.", "Raise memory request/limit, reduce footprint, or add node capacity.", "Correct command/args; ensure script is present and executable.", "Fix probe path/port/response; relax timeouts/failureThreshold; add startupProbe if needed.", "Create/bind correct resources; fix volumeMounts/volume names/keys.", "Debug application, enable core dumps, upgrade dependencies."], "related_patterns": ["crashloop", "crashloopbackoff", "restart", "oomkilled", "exit code", "liveness", "readiness", "CrashLoopBackOff", "Back-off restarting failed container", "container .* exited with code", "Liveness probe failed", "Readiness probe failed", "OOMKilled", "Error: container .* not found"]}
{"id": "common-issues", "category": "documentation", "symptoms": [], "root_cause": "### Common Kubernetes Issues Cheat Sheet", "investigation": [], "fixes": [], "related_patterns": []}
{"id": "configmap-secret-troubleshooting", "category": "troubleshooting", "symptoms": ["Pods fail to start with missing file/key errors", "Mount path empty or missing files", "Env var from ConfigMap/Secret is empty", "ConfigMap .* not found", "Secret .* not found", "no such file or directory", "permission denied", "could not find key", "Wrong name/namespace - Refers to ConfigMap/Secret that doesn’t exist in ns.", "Key mismatch - Key in volume/envFrom not present in resource.", "Permissions/fsGroup - Mounted files inaccessible to process user."], "root_cause": "Fix missing/broken ConfigMap/Secret references: wrong names/keys, mount paths, permissions.", "investigation": ["kubectl describe pod -n <ns> <pod> | sed -n '/Volumes/,$p'", "kubectl get configmap -n <ns> <name> -o yaml", "kubectl get secret -n <ns> <name> -o yaml", "kubectl logs -n <ns> <pod> --previous | head -n 100", "1) Describe pod (Volumes/env) to see referenced ConfigMap/Secret names/keys.", "2) LIST_ALL ConfigMap/Secret in ns; ensure names/keys exist.", "3) If missing keys → align items/envFrom with actual keys.", "4) If permission issues → set fsGroup/runAsUser; adjust defaultMode.", "5) Restart pod; confirm mounts/env populated and app starts.", "LIST_ALL ConfigMap/Secret; compare to pod spec.", "Inspect data keys; compare to items/env.", "Logs show permission denied; check runAsUser/fsGroup."], "fixes": ["Verify names/keys: LIST_ALL ConfigMap/Secret; ensure volumeMount/envFrom matches. Fix key names and redeploy.", "Create in correct ns or adjust reference.", "Add correct key or update pod spec to existing key.", "Set fsGroup/runAsUser; adjust file mode via defaultMode/items."], "related_patterns": ["configmap", "secret", "mount", "env", "file not found", "permission", "ConfigMap .* not found", "Secret .* not found", "no such file or directory", "permission denied", "could not find key"]}
{"id": "statefulset-troubleshooting", "category": "troubleshooting", "symptoms": ["StatefulSet pods Pending/CrashLoop", "PVCs not bound or stuck", "Scaling blocked at an ordinal", "Deletion stuck due to PVCs/finalizers", "PersistentVolumeClaim .* Pending", "volume node affinity conflict", "pod has unbound immediate PersistentVolumeClaims", "CrashLoopBackOff (stateful pod)", "PVC not bound - Per-ordinal PVC lacks matching PV/storage.", "CrashLoop in ordinal - Pod crashes, blocking next ordinal.", "Volume node affinity conflict - PV bound to node that cannot schedule pod."], "root_cause": "Resolve StatefulSet rollout issues: stuck pods, PVC binding, ordering, and deletion.", "investigation": ["kubectl get statefulset -n <ns> <name> -o wide", "kubectl describe statefulset -n <ns> <name>", "kubectl get pvc -n <ns> | grep <name>", "kubectl describe pvc -n <ns> <pvc>", "kubectl describe pod -n <ns> <pod>", "kubectl get events -n <ns> --sort-by=.lastTimestamp | tail -n 30", "1) List StatefulSet and pods: LIST_ALL StatefulSet/Pod in <ns>.", "2) Check which ordinal is blocked (Ready/CrashLoop/Pending).", "3) If PVC Pending for that ordinal → fix storage (class/size/mode/node affinity).", "4) If CrashLoop → logs/describe per crashloop guide.", "5) Verify ordinal becomes Ready; StatefulSet proceeds to next.", "6) For delete: ensure PVCs/finalizers removed if safe.", "7) Confirm all pods Ready and PVCs Bound.", "kubectl get pvc -n <ns> | grep <setname>-<ordinal>; describe pvc.", "Describe pod/logs; same as crashloop guide.", "Describe PVC/PV; check nodeAffinity on PV vs pod/node selectors."], "fixes": ["Check PVCs for each ordinal: ensure Bound; fix StorageClass/size/mode. If CrashLoop, treat like normal pod (logs/events).", "Provide PV/StorageClass; correct size/accessMode; fix node affinity.", "Fix pod crash; StatefulSet will continue.", "Adjust selectors or re-provision PV matching target nodes."], "related_patterns": ["statefulset", "pvc", "volume", "ordering", "restart", "scale", "PersistentVolumeClaim .* Pending", "volume node affinity conflict", "pod has unbound immediate PersistentVolumeClaims", "CrashLoopBackOff (stateful pod)"]}
{"id": "vcluster-troubleshooting", "category": "troubleshooting", "symptoms": ["vCluster pod not running", "Cannot connect to virtual cluster", "Resources not syncing to host", "Networking issues between vCluster and host", "vcluster list shows no clusters", "vcluster.*not found", "failed to connect.*vcluster", "syncer.*error", "cannot reach.*vcluster", "vCluster pod failing - The vCluster syncer/API server pod is crashlooping", "Kubeconfig not working - Cannot connect to virtual cluster API", "Cannot find vclusters - vcluster list shows empty or error", "Sync issues - Resources in vCluster not appearing on host", "Storage issues - vCluster needs PVC but storage not provisioned"], "root_cause": "Troubleshooting vCluster virtual Kubernetes clusters - how to find, connect, and debug vclusters", "investigation": ["vcluster list", "kubectl get vclusters -A", "kubectl get pods -A -l app=vcluster", "kubectl get pods -n <vcluster-namespace>", "kubectl logs -n <ns> -l app=vcluster", "vcluster connect <name> -n <ns> -- kubectl get pods -A", "kubectl get pvc -n <vcluster-namespace>", "1. Find vclusters: vcluster list OR kubectl get vclusters -A", "2. Check vCluster pod status in namespace", "3. Check pod logs for errors", "4. Test connection: vcluster connect <name> -n <ns>", "5. Once connected, check internal cluster state", "6. Check PVC if using persistent storage", "kubectl get pods -n <vcluster-namespace> -l app=vcluster", "vcluster connect <name> -n <ns>", "Use kubectl get vclusters -A instead", "Check syncer logs for errors"], "fixes": ["vcluster list && kubectl get vclusters -A", "Check pod logs, verify storage class, check resource limits", "Regenerate kubeconfig, verify vCluster pod is healthy", "Check if vcluster operator is installed, verify namespace", "Verify sync patterns in vcluster config, check RBAC", "Create StorageClass or use --distro k3s with in-memory"], "related_patterns": ["vcluster", "virtual", "cluster", "multitenancy", "syncer", "host", "nested", "management-cluster", "find-vclusters", "list-vclusters", "vcluster.*not found", "failed to connect.*vcluster", "syncer.*error", "cannot reach.*vcluster"]}
{"id": "pod-security-troubleshooting", "category": "troubleshooting", "symptoms": ["Pod creation/update forbidden due to security policy", "Events mention PodSecurity restricted/baseline violation", "Legacy PSP/SCC errors", "violates PodSecurity", "forbidden: unable to validate against any pod security policy", "SCC", "privileged escalation", "hostPath not allowed", "Privileged/host access blocked - Pod requests privileged, hostNetwork, hostPath.", "runAsNonRoot enforced - Pod runs as root when policy requires non-root.", "Missing PSP/SCC binding - ServiceAccount not bound to required PSP/SCC."], "root_cause": "Resolve pod admission denials from PodSecurity (restricted/baseline) or legacy PSP/SCC.", "investigation": ["kubectl get events -n <ns> --sort-by=.lastTimestamp | tail -n 20", "kubectl auth can-i create pod --as=<user> -n <ns>", "kubectl get psp", "kubectl get scc", "1) Check event/error for exact violation (privileged, hostPath, runAsNonRoot, caps).", "2) Adjust pod spec to comply: drop privileged/host*, set runAsNonRoot, drop caps, set seccompProfile.", "3) For legacy PSP/SCC: ensure SA bound to correct policy; align pod spec to policy constraints.", "4) Retry creation; confirm admission passes.", "Events show forbidden for these fields.", "Events mention runAsNonRoot violation.", "Check RBAC bindings to PSP/SCC."], "fixes": ["Align pod spec with the enforced profile (restricted/baseline): drop privileged/hostPath/capabilities; set runAsNonRoot; match allowed seccomp.", "Remove privileged/host*; use PVC; drop capabilities.", "Set runAsNonRoot: true; runAsUser to non-zero UID.", "Bind SA to appropriate PSP/SCC or migrate to PodSecurity admission."], "related_patterns": ["pod security", "psp", "pss", "scc", "forbidden", "violates PodSecurity", "forbidden: unable to validate against any pod security policy", "SCC", "privileged escalation", "hostPath not allowed"]}
{"id": "image-pull-troubleshooting", "category": "troubleshooting", "symptoms": ["Pod stuck in ImagePullBackOff/ErrImagePull", "Events show 'Failed to pull image'", "Private registry auth errors", "ImagePullBackOff", "ErrImagePull", "Failed to pull image", "unauthorized.*authentication required", "manifest.*not found", "repository does not exist", "dial .* i/o timeout", "x509: certificate signed by unknown authority", "Image/tag does not exist - Wrong image name or tag.", "Authentication required - Private registry needs credentials.", "Registry unreachable - Network/DNS/firewall blocking registry.", "TLS/CA mismatch - Custom CA or self-signed cert not trusted."], "root_cause": "Actionable checklist for image pull failures: name/tag correctness, auth, registry reachability, and pull secrets.", "investigation": ["kubectl describe pod -n <ns> <pod> | sed -n '/Events/,$p'", "kubectl get pod -n <ns> <pod> -o jsonpath='{.spec.containers[*].image}'", "kubectl get secret -n <ns> | grep -i docker", "kubectl get pod -n <ns> <pod> -o jsonpath='{.spec.imagePullSecrets}'", "kubectl debug node/<node> -it --image=ghcr.io/curl/curl -- curl -v https://<registry>/v2/", "1) Discover pod: FIND_ISSUES or LIST_ALL Pod to get <ns>/<pod>.", "2) Describe for exact error: kubectl describe pod -n <ns> <pod> | sed -n '/Events/,$p'.", "3) If unauthorized → create imagePullSecret and attach (spec.imagePullSecrets or serviceAccount).", "4) If manifest not found → verify image:tag exists; docker pull <image:tag>.", "5) If timeout/connection refused → test connectivity from node (kubectl debug node/... curl registry).", "6) If x509 → add trusted CA or use correct registry endpoint.", "7) Confirm fix: delete/restart pod; ensure status goes Running/Ready.", "Check Events for 'manifest not found'; docker pull <image:tag> from a machine with registry access.", "Events show 'unauthorized'/'authentication required'.", "Events show timeout/connection refused; test: kubectl debug/node shell -> curl <registry>.", "Events show x509 unknown authority."], "fixes": ["kubectl describe pod -n <ns> <pod> | sed -n '/Events/,$p' — fix exact image:tag and imagePullSecrets based on the event message.", "Use correct image:tag; ensure image is pushed/present in registry.", "Create imagePullSecret (type kubernetes.io/dockerconfigjson) and add to pod spec or namespace default serviceAccount.", "Open egress to registry, fix DNS, verify proxy settings.", "Add CA to nodes/docker or include ca.crt in imagePullSecret; use correct registry URL (https vs http)."], "related_patterns": ["imagepull", "imagepullbackoff", "errimagepull", "registry", "authentication", "image", "notfound", "docker", "ImagePullBackOff", "ErrImagePull", "Failed to pull image", "unauthorized.*authentication required", "manifest.*not found", "repository does not exist", "dial .* i/o timeout", "x509: certificate signed by unknown authority"]}
{"id": "uipath-managementcluster-troubleshooting", "category": "troubleshooting", "symptoms": ["Not reaching Ready state - ManagementCluster stuck in provisioning", "vCluster issues - Underlying vCluster not healthy", "Resource constraints - Not enough resources on host cluster"], "root_cause": "Troubleshooting UiPath ManagementCluster custom resources (dedicated.uipath.com)", "investigation": ["kubectl get managementcluster -A", "kubectl describe managementcluster <name> -n <namespace>", "kubectl get pods -n <namespace> -l app=vcluster", "kubectl logs -n <namespace> -l app=vcluster", "kubectl get clusters.cluster.x-k8s.io -n <namespace>", "kubectl describe nodes, kubectl top nodes"], "fixes": ["1. Check ManagementCluster state: kubectl get managementcluster -n <namespace>", "2. Describe for conditions and events", "3. Verify vCluster pods are running", "4. Check vCluster connectivity: vcluster connect", "5. Review controller logs for specific errors", "Check vCluster pod status, verify host cluster resources", "Check vCluster syncer logs, verify PVC storage", "Scale host cluster or reduce management cluster resource requests"], "related_patterns": ["managementcluster", "uipath", "dedicated", "vcluster", "management", "cluster"]}
{"id": "kb-index", "category": "troubleshooting", "symptoms": [], "root_cause": "See troubleshooting entry kb-index.", "investigation": [], "fixes": [], "related_patterns": []}
{"id": "resource-limits-troubleshooting", "category": "troubleshooting", "symptoms": ["Pods throttled or slow", "Evictions under memory pressure", "Pending due to Insufficient cpu/memory", "OOMKilled", "evicted", "ExceededGracePeriod", "cpu throttling", "Out of memory", "memory pressure", "Insufficient cpu/memory", "Under-requesting/overusing resources - Actual usage > limits/requests.", "Node-level pressure - Nodes out of memory/CPU; kubelet evicts.", "Aggressive limits causing throttling - CPU limit too low; throttling."], "root_cause": "Diagnose CPU throttling, memory pressure, and evictions by comparing usage vs requests/limits and node capacity.", "investigation": ["kubectl top pod -n <ns> <pod>", "kubectl get pod -n <ns> <pod> -o jsonpath='{.spec.containers[*].resources}'", "kubectl top nodes", "kubectl describe pod -n <ns> <pod> | sed -n '/Events/,$p'", "kubectl get events -A --sort-by=.lastTimestamp | tail -n 30", "1) Check usage: kubectl top pod -n <ns> <pod> (CPU/mem).", "2) Check spec: kubectl get pod -n <ns> <pod> -o yaml | grep -A5 resources.", "3) If usage > limits → increase limits/requests or optimize.", "4) If node pressure/evictions → check top nodes; add capacity or rebalance.", "5) If Pending Insufficient cpu/memory → lower requests or add nodes.", "6) Validate: monitor after changes; ensure no throttling/evictions.", "kubectl top pod vs spec resources.", "kubectl top nodes; events show eviction due to pressure.", "Metrics show throttling; high CPU usage vs low limits."], "fixes": ["Check pod usage vs requests/limits (kubectl top pod; get pod -o yaml | grep resources). Adjust requests/limits to match reality; add capacity if nodes are saturated.", "Adjust requests/limits to match real usage; optimize app.", "Add nodes; rebalance; set realistic limits to avoid eviction.", "Increase CPU limit; tune requests; enable HPA."], "related_patterns": ["cpu", "memory", "limits", "requests", "eviction", "throttling", "oom", "evicted", "ExceededGracePeriod", "cpu throttling", "Out of memory", "memory pressure", "Insufficient cpu/memory"]}
{"id": "certificate-tls-troubleshooting", "category": "troubleshooting", "symptoms": ["x509 unknown authority", "TLS handshake failure", "Webhook errors about certificate", "Clients cannot connect via HTTPS", "x509: certificate signed by unknown authority", "certificate has expired", "certificate is not valid for", "remote error: tls", "handshake failure", "Wrong or missing CA trust - Client/webhook CABundle lacks correct CA.", "SAN/hostname mismatch - Certificate CN/SANs do not match service DNS.", "Expired cert - Serving/client cert expired."], "root_cause": "Fix x509/certificate errors for services/webhooks/clients: CA trust, SANs, expiry, and mTLS.", "investigation": ["kubectl describe secret -n <ns> <tls-secret>", "kubectl get validatingwebhookconfiguration -o yaml | grep -A3 caBundle", "kubectl logs -n <ns> <pod> | grep -i x509", "kubectl run tls-check --image=ghcr.io/curl/curl --restart=Never -- curl -v https://<svc>.<ns>.svc", "1) Capture exact TLS error (unknown authority/SAN/expired).", "2) If webhook: verify CABundle matches serving cert CA; patch if needed.", "3) If SAN mismatch: regenerate cert with correct DNS/IP SANs for service.", "4) If expired: renew cert/secret; restart dependent pods.", "5) For mTLS: ensure both client/server trust CAs and present required client certs.", "6) Retest connection; ensure no TLS errors.", "Webhook errors x509 unknown authority; client logs.", "Error 'certificate is not valid for <host>'.", "kubectl describe secret; check NotAfter; logs mention expired."], "fixes": ["Ensure the client trusts the serving CA (CABundle for webhooks), cert SANs match hostnames, and certs are not expired.", "Patch CABundle; add CA to trust; restart using components.", "Regenerate cert with correct DNS/IP SANs (service.namespace.svc).", "Renew/rotate cert; restart pods to pick it up."], "related_patterns": ["tls", "certificate", "x509", "https", "mtls", "x509: certificate signed by unknown authority", "certificate has expired", "certificate is not valid for", "remote error: tls", "handshake failure"]}
{"id": "kb-101", "category": "reference", "symptoms": [], "root_cause": "Reference for diagnosing and fixing connectivity and authentication issues when talking to a Kubernetes cluster", "investigation": [], "fixes": [], "related_patterns": ["kubectl", "cluster", "authentication", "connectivity", "contexts"]}
{"id": "kb-102", "category": "reference", "symptoms": [], "root_cause": "Diagnosis and remediation for node-level problems and pod scheduling failures", "investigation": [], "fixes": [], "related_patterns": ["nodes", "scheduling", "pending-pods", "cluster-capacity"]}
{"id": "kb-103", "category": "reference", "symptoms": [], "root_cause": "Detailed guidance for common pod lifecycle and container runtime failures", "investigation": [], "fixes": [], "related_patterns": ["pods", "containers", "crashloopbackoff", "imagepullbackoff"]}
{"id": "kb-104", "category": "reference", "symptoms": [], "root_cause": "Playbooks for service discovery, connectivity, and ingress-related failures", "investigation": [], "fixes": [], "related_patterns": ["networking", "services", "ingress", "dns", "cni"]}
{"id": "kb-105", "category": "reference", "symptoms": [], "root_cause": "Troubleshooting guide for PersistentVolumeClaims, storage classes, and volume mount failures", "investigation": [], "fixes": [], "related_patterns": ["storage", "pvc", "pv", "csi", "volumes"]}
{"id": "kb-106", "category": "reference", "symptoms": [], "root_cause": "Troubleshooting slow or failing API server operations, etcd problems, and throttling", "investigation": [], "fixes": [], "related_patterns": ["control-plane", "apiserver", "etcd", "rate-limits"]}
{"id": "kb-107", "category": "reference", "symptoms": [], "root_cause": "Systematic approach to authorization errors, admission webhook failures, and resource quota issues", "investigation": [], "fixes": [], "related_patterns": ["rbac", "admission", "webhook", "quota", "policy"]}
{"id": "kb-108", "category": "reference", "symptoms": [], "root_cause": "Common error patterns and workflows for core Kubernetes workloads", "investigation": [], "fixes": [], "related_patterns": ["deployments", "jobs", "cronjobs", "daemonsets", "workloads"]}
{"id": "castai-troubleshooting", "category": "troubleshooting", "symptoms": ["Autoscaling not adding nodes", "Provisioning failures", "Spot/cluster imbalance", "castai-controller", "failed to provision", "quota", "insufficient capacity", "cloud credentials", "Controller down - castai-controller not running.", "Cloud quota/capacity - Provider quota exhausted or no capacity for requested type.", "Credentials/permissions - Cloud creds invalid/insufficient."], "root_cause": "Investigate CAST AI autoscaling/provisioning issues: controllers, cloud creds, quotas, and node health.", "investigation": ["kubectl get pods -n castai-system", "kubectl logs -n castai-system deploy/castai-controller | tail -n 100", "kubectl get events -n castai-system --sort-by=.lastTimestamp | tail -n 30", "kubectl get nodes", "1) Check castai-system pods/events for controller health.", "2) Review controller logs for quota/auth/capacity errors.", "3) Verify cloud credentials/permissions and quotas.", "4) If nodes provisioned but NotReady → debug node readiness (CNI/kubelet).", "5) Confirm autoscaling adds ready nodes; workloads schedule.", "kubectl get pods -n castai-system.", "Logs show quota/insufficient capacity.", "Controller logs show auth errors."], "fixes": ["Check castai-controller logs/events; ensure cloud credentials and quotas are sufficient; verify node provisioning status.", "Restart/fix controller deployment.", "Increase quotas; use different instance sizes/zones.", "Update credentials/permissions in CAST AI config."], "related_patterns": ["castai", "autoscaling", "provisioning", "nodes", "spot", "castai-controller", "failed to provision", "quota", "insufficient capacity", "cloud credentials"]}
{"id": "crossplane-troubleshooting", "category": "troubleshooting", "symptoms": ["Managed resource stuck in Creating", "Provider shows Unhealthy", "Synced: False on resources", "Ready: False after long time", "cannot.*provision", "provider.*not healthy", "Synced.*False", "Ready.*False", "cannot resolve.*reference", "Provider not healthy - Provider controller pod is failing", "Invalid credentials - ProviderConfig has wrong or expired credentials", "Cloud API quota exceeded - Cloud provider rejects creation due to quotas", "Reference not found - Managed resource references non-existent resource"], "root_cause": "Troubleshooting Crossplane providers and managed infrastructure resources", "investigation": ["kubectl get providers", "kubectl get managed", "kubectl describe <managed-resource> -n <ns>", "kubectl get providerconfig", "kubectl logs -n crossplane-system -l app=crossplane", "kubectl get pods -n crossplane-system", "1. GET_CROSSPLANE to see all providers and managed resources", "2. Check provider health status", "3. Describe failing managed resource for events", "4. Check ProviderConfig credentials", "5. Verify cloud quotas and permissions", "kubectl get providers, check pod status in crossplane-system", "Events show 'unauthorized' or 'access denied'", "Describe managed resource, check events for quota errors", "Events show 'cannot resolve reference'"], "fixes": ["kubectl get managed && kubectl get providers", "Check provider pod logs, verify ProviderConfig credentials", "Update Secret referenced by ProviderConfig", "Request quota increase from cloud provider", "Create referenced resource first or fix reference"], "related_patterns": ["crossplane", "managed", "provider", "claim", "xr", "composite", "azure", "aws", "gcp", "infrastructure", "cannot.*provision", "provider.*not healthy", "Synced.*False", "Ready.*False", "cannot resolve.*reference"]}
{"id": "helm-troubleshooting", "category": "troubleshooting", "symptoms": ["helm install/upgrade fails or times out", "Release stuck in pending/failed state", "CRD already exists errors", "failed post-install", "failed pre-upgrade", "timed out waiting for", "cannot patch .*", "CRD .* already exists", "UPGRADE FAILED", "Hook job/pod failed - Helm hook pod fails and blocks release.", "CRD conflict - CRD already exists or version mismatch.", "Resource immutable fields - Upgrade tries to change immutable fields."], "root_cause": "Resolve Helm install/upgrade issues: failed hooks, CRD ordering, timeouts, and rollbacks.", "investigation": ["helm list -A", "helm status <release> -n <ns>", "kubectl get pods -A | grep <release>", "kubectl logs -n <ns> <hook-pod> --previous", "kubectl get events -A --sort-by=.lastTimestamp | tail -n 50", "1) helm status <release> -n <ns>; note failing hooks/resources.", "2) Inspect hook pods/jobs: kubectl get pods -A | grep <release>; logs/events for failures.", "3) If CRD conflict: install/upgrade CRDs separately; rerun with --skip-crds.", "4) If timeout: find stuck pods/resources; fix CrashLoop/Readiness/NP issues; rerun helm.", "5) If immutable errors: delete/recreate offending resource or adjust values.", "6) Confirm release deployed; helm status shows deployed/success.", "kubectl get pods -A | grep hook; logs/events for hook pods.", "helm error mentions CRD exists; kubectl get crd <name>.", "Helm output mentions immutable; check resource diff."], "fixes": ["If hooks timed out, inspect hook jobs/pods; delete failed hooks and rerun. For CRD conflicts, install/upgrade CRDs separately with --skip-crds or --include-crds as appropriate.", "Fix hook failure; delete failed hook pod/job; rerun helm.", "Manage CRDs separately; use --skip-crds or apply CRDs first.", "Delete/recreate resource or adjust values to avoid immutable change."], "related_patterns": ["helm", "install", "upgrade", "rollback", "crd", "failed post-install", "failed pre-upgrade", "timed out waiting for", "cannot patch .*", "CRD .* already exists", "UPGRADE FAILED"]}
{"id": "uipath-asea-troubleshooting", "category": "troubleshooting", "symptoms": ["ASEA deployment fails in Azure", "Cannot access UiPath portal after deployment", "AKS node pool issues", "Storage or networking problems", "Certificate renewal failures", "Upgrade or patching issues", "AKS node pool scaling issues - Nodes not scaling or stuck in provisioning", "Application Gateway misconfiguration - Ingress not routing traffic correctly", "Key Vault access denied - Pods cannot access secrets from Key Vault", "Azure Files mount failures - PVCs failing to mount Azure Files shares", "DNS resolution issues - Cannot resolve UiPath endpoints", "Certificate expiration - TLS certificates expired, causes auth failures"], "root_cause": "Troubleshooting UiPath Automation Suite Express deployed via Azure Marketplace on AKS", "investigation": ["kubectl get nodes -o wide", "kubectl get pods -n uipath -o wide", "az aks show -n <cluster> -g <rg> --query 'provisioningState'", "az aks nodepool list -g <rg> --cluster-name <cluster>", "kubectl get ingress -n uipath", "kubectl logs -n uipath <pod>", "kubectl get pvc -n uipath", "az keyvault secret list --vault-name <vault>", "az aks nodepool list, kubectl get nodes", "Check App Gateway health probes, backend pool status", "Check managed identity, Key Vault access policies", "kubectl describe pvc, check mount errors in pod events", "Check Azure Private DNS zones, nslookup from pod", "kubectl get certificates, check Key Vault cert expiry"], "fixes": ["1. Verify AKS cluster health: az aks show", "2. Check all UiPath pods are running", "3. Verify Application Gateway health probes pass", "4. Check Key Vault access from pods", "5. Review Azure Activity Log for resource errors", "6. Check certificate validity and DNS resolution", "Check Azure quotas, VM availability in region, scale manually if needed", "Verify backend pool IPs, check probe paths, review WAF rules", "Grant identity access to Key Vault, verify CSI driver", "Verify storage account connectivity, firewall rules, CSI driver", "Verify DNS zone links, check VNet peering if applicable", "Renew certificates in Key Vault, trigger cert-manager sync"], "related_patterns": ["uipath", "asea", "azure", "automation", "suite", "express", "aks", "marketplace"]}
{"id": "dns-networking-troubleshooting", "category": "troubleshooting", "symptoms": ["DNS lookups return NXDOMAIN/SERVFAIL/no such host", "Pods cannot reach Services (timeout/connection refused)", "Only some namespaces/pods affected", "no such host", "i/o timeout", "connection refused", "SERVFAIL", "NXDOMAIN", "dns lookup .* failed", "CoreDNS not running/health issues - CoreDNS pods CrashLoop or not Ready.", "No endpoints for Service - Service points to zero Ready pods.", "NetworkPolicy blocking DNS or app traffic - Policies deny egress to DNS or ingress to app.", "CNI issues - Overlay/host networking broken on nodes.", "Bad DNS config/upstream - Stub domains/upstream resolvers unreachable."], "root_cause": "Diagnose DNS resolution failures and pod-to-service connectivity (timeouts/connection refused) by checking CoreDNS, endpoints, CNI, and NetworkPolicy.", "investigation": ["kubectl get pods -n kube-system -l k8s-app=kube-dns", "kubectl logs -n kube-system deploy/coredns | tail -n 50", "kubectl get endpoints -n <ns> <svc>", "kubectl describe svc -n <ns> <svc>", "kubectl run -it dns-diag --image=ghcr.io/curl/curl --restart=Never -- nslookup <svc>.<ns>.svc.cluster.local", "kubectl get networkpolicy -n <ns> -o yaml", "1) Check CoreDNS: kubectl get pods -n kube-system -l k8s-app=kube-dns; ensure Running/Ready.", "2) Test DNS from a debug pod: kubectl run -it dns-diag --image=ghcr.io/curl/curl --restart=Never -- nslookup <svc>.<ns>.svc.cluster.local.", "3) Endpoints: GET_ENDPOINTS <ns> <svc>; if none, fix selectors/readiness/scale.", "4) NetworkPolicy: inspect policies in <ns>; ensure egress to kube-dns:53 and ingress/egress between client and pods.", "5) Ports: verify Service port/targetPort matches containerPort.", "6) If cross-node issues: check CNI pods/logs in kube-system; ensure node routes and IPAM healthy.", "7) Confirm: retry nslookup/curl; Service reachable without 503/timeouts.", "kubectl get pods -n kube-system -l k8s-app=kube-dns; describe CoreDNS pods.", "GET_ENDPOINTS shows 0 addresses; pods CrashLoop/NotReady.", "Review NetworkPolicies in ns; test from allowed namespace.", "Pod-to-pod ping/HTTP fails across nodes; check CNI pods (kube-system).", "CoreDNS logs show upstream errors; external lookups fail."], "fixes": ["Check CoreDNS is Running, verify Service endpoints exist, then nslookup/curl from a debug pod. If NetworkPolicy blocks, allow DNS (53/udp,tcp) and app ports.", "Ensure resources, fix image/configmap, restart deployment.", "Fix pod health/readiness; align selectors.", "Allow UDP/TCP 53 to kube-dns and app ports between client/pod namespaces.", "Restart CNI pods; check node routing; ensure IP pools not exhausted.", "Fix stubDomain/upstream resolvers; allow egress."], "related_patterns": ["dns", "networking", "service", "endpoint", "cni", "kube-dns", "core-dns", "connection refused", "timeout", "no such host", "i/o timeout", "SERVFAIL", "NXDOMAIN", "dns lookup .* failed"]}
{"id": "namespace-stuck-terminating", "category": "troubleshooting", "symptoms": ["kubectl delete ns <ns> hangs", "Namespace stuck in Terminating", "Resources with finalizers remain", "Namespace .* is being terminated", "finalizer", "cannot delete", "stuck terminating", "Finalizer left on resource - CRDs/objects with finalizers block namespace deletion.", "Controller absent - Owning controller removed/crashed.", "External dependency - Cloud resource deletion blocked."], "root_cause": "Clear namespaces stuck in Terminating by finding blocking finalizers/resources and addressing controllers safely.", "investigation": ["kubectl get namespace <ns> -o yaml", "kubectl get all -n <ns>", "LIST_FINALIZERS <ns>", "kubectl get events -n <ns> --sort-by=.lastTimestamp | tail -n 20", "1) Inspect namespace: kubectl get ns <ns> -o yaml (finalizers/status).", "2) List blocking resources: LIST_FINALIZERS <ns>; kubectl get all -n <ns>.", "3) Ensure controllers running for blocking resources (e.g., CRD controllers).", "4) If safe, patch finalizers=[] on stuck resources; otherwise fix controller and let it clean.", "5) If CRD removed, recreate CRD temporarily to delete remaining CRs.", "6) Retry delete namespace; confirm it disappears.", "LIST_FINALIZERS <ns>; kubectl get <kind> -n <ns> -o json | grep finalizers.", "Check controller deployment; events show failed cleanup.", "Events/logs mention cloud API errors."], "fixes": ["Find resources with finalizers (LIST_FINALIZERS <ns>), delete or patch offending resources/finalizers if safe; ensure controllers are running.", "Let controller run; or patch finalizers=[] if safe.", "Restore controller; then delete resources cleanly.", "Resolve cloud-side issue; retry; as last resort patch finalizer."], "related_patterns": ["namespace", "terminating", "finalizer", "deletion", "Namespace .* is being terminated", "cannot delete", "stuck terminating"]}
{"id": "kb-004", "category": "reference", "symptoms": [], "root_cause": "Comprehensive kubectl command reference for debugging vCluster environments", "investigation": [], "fixes": [], "related_patterns": ["commands", "reference", "vcluster", "kubectl"]}
{"id": "probe-troubleshooting", "category": "troubleshooting", "symptoms": ["Pods restart repeatedly without clear app errors", "Ready 0/1 due to readiness failures", "Liveness kills container before startup completes", "Liveness probe failed", "Readiness probe failed", "startup probe failed", "Probe terminated the container", "http probe failed with statuscode 503", "context deadline exceeded", "Probe timing too aggressive - Liveness/readiness starts before app is ready.", "Wrong path/port/scheme - Probe hits wrong endpoint/port (404/503).", "Sidecar readiness - Envoy/sidecar not ready; app blocked.", "Resource starvation - CPU/memory pressure delays responses."], "root_cause": "Fix probe failures causing restarts or unready pods: timing, paths/ports, sidecars, and slow starts.", "investigation": ["kubectl describe pod -n <ns> <pod> | sed -n '/Events/,$p'", "kubectl logs -n <ns> <pod> --previous | head -n 120", "kubectl get pod -n <ns> <pod> -o jsonpath='{.spec.containers[*].livenessProbe}'", "kubectl top pod -n <ns> <pod>", "kubectl get events -n <ns> --sort-by=.lastTimestamp | tail -n 20", "1) Describe pod events for probe failures.", "2) Check probe config (path/port/scheme/initialDelay/failureThreshold).", "3) If slow start → add startupProbe and relax timings.", "4) If 404/503 → fix path/port; ensure app listens; consider sidecar readiness.", "5) If timeouts + high CPU/mem → increase resources or reduce load.", "6) Validate: restart pod; ensure Ready 1/1, restarts stable.", "Describe pod events show early probe failures; app starts slowly.", "Probe path differs from app; logs show 404/503.", "Probes fail until sidecar ready; service mesh present.", "TOP_PODS shows high CPU/mem; timeouts in probe."], "fixes": ["Increase initialDelaySeconds/failureThreshold, verify probe path/port, and add startupProbe for slow starts.", "Increase initialDelaySeconds; add startupProbe; increase failureThreshold/periodSeconds.", "Align probe path/port/scheme with app; ensure containerPort matches targetPort.", "Use appropriate readiness gate or wait for sidecar; check mesh docs.", "Increase resources; reduce load; optimize app."], "related_patterns": ["probe", "liveness", "readiness", "startupProbe", "healthcheck", "503", "Liveness probe failed", "Readiness probe failed", "startup probe failed", "Probe terminated the container", "http probe failed with statuscode 503", "context deadline exceeded"]}
{"id": "rbac-permissions-troubleshooting", "category": "troubleshooting", "symptoms": ["Forbidden/Unauthorized on kubectl or pod actions", "API calls return 403", "Pods failing due to insufficient permissions", "forbidden", "unauthorized", "User .* cannot", "system:serviceaccount", "RBAC: access denied", "Missing binding - Role/ClusterRole not bound to user/SA.", "Wrong scope - Role (namespaced) used for cluster resources, or vice versa.", "ServiceAccount not set - Workload uses default SA lacking permissions.", "Missing verbs/resources - Role rules lack required verbs."], "root_cause": "Diagnose and fix RBAC errors (Forbidden/Unauthorized) for users and service accounts with concrete can-i tests and binding checks.", "investigation": ["kubectl auth can-i <verb> <resource> --as=system:serviceaccount:<ns>:<sa>", "kubectl auth can-i --list --as=<user>", "kubectl get rolebinding,clusterrolebinding -A | grep <sa/user>", "kubectl describe role <role> -n <ns>", "kubectl describe clusterrole <cr>", "1) Identify actor and failing verb/resource from the error.", "2) Test permissions: kubectl auth can-i <verb> <resource> --as=<actor>.", "3) If forbidden → check Role/ClusterRole existence and scope.", "4) Add/adjust RoleBinding/ClusterRoleBinding to grant required verbs/resources.", "5) Verify: rerun auth can-i; retry the operation.", "kubectl get rolebinding,clusterrolebinding -A | grep <sa/user>.", "Forbidden on cluster-scoped resources; using Role instead of ClusterRole.", "kubectl get pod -o jsonpath='{.spec.serviceAccountName}'.", "kubectl describe role/clusterrole; check rules."], "fixes": ["Use kubectl auth can-i <verb> <resource> --as=system:serviceaccount:<ns>:<sa> to see what’s missing; add/adjust RoleBinding/ClusterRoleBinding accordingly.", "Create appropriate RoleBinding/ClusterRoleBinding.", "Use ClusterRole + ClusterRoleBinding for cluster-wide resources.", "Create SA with proper bindings; set serviceAccountName.", "Add needed verbs/resources/groups; reapply binding."], "related_patterns": ["rbac", "permissions", "forbidden", "unauthorized", "serviceaccount", "role", "clusterrole", "rolebinding", "User .* cannot", "system:serviceaccount", "RBAC: access denied"]}
{"id": "kb-205", "category": "troubleshooting", "symptoms": [], "root_cause": "Guide for debugging service mesh issues including sidecar injection, mTLS, and traffic policies", "investigation": [], "fixes": [], "related_patterns": ["istio", "linkerd", "service-mesh", "sidecar", "envoy", "mtls", "traffic"]}
{"id": "scheduling-troubleshooting", "category": "troubleshooting", "symptoms": ["Pod stuck in Pending", "Events show FailedScheduling", "Messages like 'Insufficient cpu/memory', 'didn't match node selector', 'had taint'", "0/\\d+ nodes are available", "Insufficient (cpu|memory)", "didn't match Pod's node selector", "node(s) had taint", "node(s) didn't match Pod's (node affinity|node selector)", "preemption", "persistentvolumeclaim .* not found", "no persistent volumes available", "Insufficient resources - No node meets requested CPU/memory.", "Selector/Affinity mismatch - Pod constraints exclude all nodes.", "Taints without tolerations - Nodes tainted; pod lacks tolerations.", "PVC not bound - Volume claim pending or missing.", "Priority/preemption - Higher priority pods block scheduling."], "root_cause": "Detailed steps to resolve Pending pods: capacity, selectors/affinity, taints/tolerations, PVC binding, priorities.", "investigation": ["kubectl describe pod -n <ns> <pod> | sed -n '/Events/,$p'", "kubectl get nodes --show-labels", "kubectl describe node <node> | sed -n '/Taints/,+5p'", "kubectl describe node <node> | sed -n '/Allocated resources/,+10p'", "kubectl get pvc -n <ns> && kubectl describe pvc -n <ns> <pvc>", "kubectl top nodes && kubectl top pods -A | head -n 30", "1) Identify pod: FIND_ISSUES or LIST_ALL Pod to get <ns>/<pod>.", "2) Describe pod for scheduling reason: kubectl describe pod -n <ns> <pod> | sed -n '/Events/,$p'.", "3) If Insufficient cpu/memory → right-size requests/limits or add capacity; verify with kubectl top nodes/pods.", "4) If selector/affinity mismatch → compare spec.nodeSelector/affinity to kubectl get nodes --show-labels; adjust labels or rules.", "5) If taints → kubectl describe node <node> (Taints); add tolerations or use other nodes.", "6) If PVC issues → kubectl get/describe pvc -n <ns>; ensure Bound status and correct name/class/size.", "7) If preemption/priority → review priorityClassName; increase capacity or lower competing priorities.", "8) Re-run scheduling (delete pod or rollout restart) after fixes; confirm pod goes Running/Ready.", "Events show 'Insufficient cpu/memory'; check requests/limits vs node allocatable.", "Events mention node selector/affinity mismatch; compare pod spec to node labels.", "Events show 'had taint'; inspect node taints.", "Events mention PVC not found/not bound; kubectl get pvc shows Pending.", "Events include preemption; check priorityClassName."], "fixes": ["kubectl describe pod -n <ns> <pod> | sed -n '/Events/,$p' — follow the scheduling reason; adjust resources/selectors/taints accordingly.", "Lower requests, right-size workloads, or add nodes.", "Update node labels or relax selectors/affinity.", "Add matching tolerations or schedule to untainted nodes.", "Create/bind PV with matching size/class/mode; correct PVC name in pod spec.", "Adjust priorities or add capacity."], "related_patterns": ["scheduling", "pending", "unschedulable", "nodeaffinity", "taint", "toleration", "resources", "nodeselector", "pvc", "0/\\d+ nodes are available", "Insufficient (cpu|memory)", "didn't match Pod's node selector", "node(s) had taint", "node(s) didn't match Pod's (node affinity|node selector)", "preemption", "persistentvolumeclaim .* not found", "no persistent volumes available"]}
{"id": "istio-gateway-troubleshooting", "category": "troubleshooting", "symptoms": ["503 errors from gateway", "404 errors for routes that should exist", "Connection refused", "Requests timing out", "Traffic not reaching pods", "503 Service Unavailable", "404 Not Found", "upstream connect error", "no healthy upstream", "connection refused", "gateway.*not found", "503 - No healthy upstream - Gateway can't reach backend pods", "404 - Route not matched - VirtualService route doesn't match request", "Gateway not selecting VirtualService - VirtualService.gateways doesn't include the Gateway", "mTLS mismatch - Client not using TLS when service requires it", "Sidecar not injected - Pod missing Istio proxy"], "root_cause": "Troubleshooting Istio service mesh, gateways, and traffic routing issues", "investigation": ["kubectl get gateways -A", "kubectl get virtualservices -A", "kubectl get destinationrules -A", "kubectl get pods -n istio-system", "kubectl describe gateway <name> -n <ns>", "kubectl describe virtualservice <name> -n <ns>", "istioctl analyze", "istioctl proxy-status", "1. GET_ISTIO to see gateways and virtual services", "2. Check istio-system pods are healthy", "3. Describe gateway and virtualservice", "4. Verify VirtualService.gateways includes your gateway", "5. Check service endpoints exist", "6. Verify sidecar injection", "Check endpoints exist and pods are ready", "Check VirtualService hosts and match rules", "Compare Gateway.metadata.name with VirtualService.gateways", "Check PeerAuthentication and DestinationRule TLS mode", "Check pod has istio-proxy container"], "fixes": ["kubectl get gateways,virtualservices -A && kubectl get pods -n istio-system", "Verify service selector matches pod labels, check pod health", "Align VirtualService with actual request path/host", "Add gateway name to VirtualService.spec.gateways", "Align TLS settings between client and server", "Label namespace with istio-injection=enabled, restart pods"], "related_patterns": ["istio", "gateway", "virtualservice", "envoy", "proxy", "503", "404", "routing", "mesh", "ingress", "503 Service Unavailable", "404 Not Found", "upstream connect error", "no healthy upstream", "connection refused", "gateway.*not found"]}
{"id": "uipath-customercluster-troubleshooting", "category": "troubleshooting", "symptoms": ["ASFailed", "InfraInProgress", "CustomerCluster.*Failed", "ManagementCluster.*not ready", "provisioning.*failed", "ASFailed state - Automation Suite installation failed during deployment", "InfraInProgress stuck > 45 min - Infrastructure provisioning stuck or taking too long", "ManagementCluster not ready - Dependency on ManagementCluster being healthy", "Node provisioning failure - Worker nodes failed to provision in Azure", "vCluster syncer issues - ManagementCluster's vCluster not syncing properly"], "root_cause": "Troubleshooting UiPath CustomerCluster CRD (dedicated.uipath.com/v1alpha1) - covers ASFailed, InfraInProgress, and provisioning issues", "investigation": ["kubectl get customercluster -A", "kubectl describe customercluster <name> -n <namespace>", "kubectl get managementcluster -A", "kubectl describe managementcluster <name> -n <namespace>", "kubectl get clusters.cluster.x-k8s.io -n <namespace>", "kubectl get machines -n <namespace>", "kubectl get machinedeployments -n <namespace>", "VCLUSTER_CMD <ns> management-cluster get pods -A", "1. Check CustomerCluster state: GET_UIPATH_CRD", "2. Check ManagementCluster state (dependency)", "3. Check underlying CAPI Cluster: GET_CAPI", "4. If InfraInProgress: check Machines and MachineDeployments", "5. If ASFailed: connect to vCluster and check AS pods", "6. Review controller logs for specific errors", "kubectl get managementcluster -n <namespace>"], "fixes": ["kubectl get customercluster -A && kubectl get managementcluster -A", "Check AS pod logs on target cluster, verify prerequisites, check resource availability", "Check ManagementCluster health, verify Azure quotas, check MachineDeployments", "Wait for ManagementClusterReady state, troubleshoot vCluster if needed", "Check Azure VM quotas, spot availability, network config, NSG rules", "Check vCluster pod logs, PVC storage"], "related_patterns": ["customercluster", "uipath", "dedicated", "cluster", "asfailed", "infrainprogress", "provisioning", "automation-suite", "crd", "ASFailed", "InfraInProgress", "CustomerCluster.*Failed", "ManagementCluster.*not ready", "provisioning.*failed"]}
{"id": "placeholder-errors", "category": "troubleshooting", "symptoms": ["Tool output says arguments contain placeholders", "Commands use [brackets], <angles>, 'example-' prefixes", "NotFound/unknown resource errors", "Investigation stops before running real tools", "\\[.*name.*\\]", "<.*name.*>", "PLACEHOLDER ERROR", "contains [brackets] or <angles>", "not a real resource name", "Invalid argument .* placeholder", "not found", "unknown resource", "Placeholders used literally - Copied examples without replacing placeholders.", "Wrong namespace/kind - Resource exists elsewhere or is a different kind.", "Stale/terminated resource - Target no longer exists."], "root_cause": "Recover quickly when commands include placeholders ([pod-name], <namespace>) or return NotFound due to wrong names.", "investigation": ["1) LIST_ALL Pod (or relevant kind) to collect <ns>/<name>.", "2) FIND_ISSUES to surface unhealthy resources.", "3) Rerun DESCRIBE/GET_LOGS with real <ns>/<name>.", "4) If still NotFound, verify namespace/kind; refresh LIST_ALL."], "fixes": ["Run FIND_ISSUES or LIST_ALL <kind> to get real names, then rerun DESCRIBE/GET_LOGS with <ns>/<name>.", "Discover names via LIST_ALL/FIND_ISSUES; rerun commands with actual ns/name.", "Check namespaces; list by kind; use correct <kind> <ns> <name>.", "List current resources and target the live ones."], "related_patterns": ["placeholder", "invalid-name", "arguments", "kubectl", "tooling", "\\[.*name.*\\]", "<.*name.*>", "PLACEHOLDER ERROR", "contains [brackets] or <angles>", "not a real resource name", "Invalid argument .* placeholder", "not found", "unknown resource"]}
{"id": "webhook-troubleshooting", "category": "troubleshooting", "symptoms": ["kubectl apply/create fails with webhook error", "Timeout/x509 when talking to webhook service", "Cluster installs/CRDs blocked", "failed calling webhook", "no such host", "x509: certificate signed by unknown authority", "context deadline exceeded", "connection refused", "Webhook service not running - Service/Deployment missing or crashed.", "Bad TLS/CABundle - API server cannot validate webhook cert.", "NetworkPolicy/port issues - API server cannot reach webhook over network/port.", "Stale webhook after uninstall - Webhook config remains but backend removed."], "root_cause": "Resolve webhook errors (timeouts, x509, connection refused) blocking creates/updates.", "investigation": ["kubectl get validatingwebhookconfiguration,mutatingwebhookconfiguration", "kubectl describe validatingwebhookconfiguration <name>", "kubectl get svc -A | grep webhook", "kubectl get endpoints -A | grep webhook", "kubectl logs -n <ns> deploy/<webhook-deploy> | tail -n 100", "1) Capture exact webhook name from error.", "2) List/describe webhook configuration to see service URL/CABundle.", "3) Check service/endpoints/pods for the webhook; ensure Ready.", "4) If x509 → patch CABundle with correct CA; restart webhook pods.", "5) If timeout/no endpoints → fix service/pod or remove stale webhook.", "6) Retry the blocked operation; confirm no webhook errors.", "kubectl get svc/deploy for webhook target; endpoints empty.", "Error shows x509 unknown authority; check CABundle in webhook config.", "Timeouts; endpoints exist; check NetworkPolicy blocking apiserver->webhook.", "No matching service/deploy; errors mention old webhook."], "fixes": ["Identify failing webhook from error; ensure service/pod exists and certs are valid; patch or delete webhook if owner app is gone (with caution).", "Restore/redeploy webhook; or remove webhook config if app is gone.", "Patch CABundle with correct CA; restart webhook pods.", "Allow traffic to webhook port; verify service port/targetPort.", "Remove webhook configs if the app is uninstalled and safe to do so."], "related_patterns": ["webhook", "admission", "tls", "timeout", "certificate", "api-server", "failed calling webhook", "no such host", "x509: certificate signed by unknown authority", "context deadline exceeded", "connection refused"]}
{"id": "kb-005", "category": "methodology", "symptoms": [], "root_cause": "Proven investigation patterns and methodologies for debugging Kubernetes and cloud infrastructure issues", "investigation": [], "fixes": [], "related_patterns": ["debugging", "best-practices", "methodology", "incident-response"]}
{"id": "autonomous-playbook", "category": "playbook", "symptoms": ["general health check", "cluster debugging", "find all issues", "autonomous scan"], "root_cause": "Step-by-step master guide for autonomous agents to debug the entire cluster without human intervention.", "investigation": ["FIND_ISSUES", "GET_EVENTS", "CLUSTER_HEALTH", "TOP_PODS --all-namespaces", "1. HIGH LEVEL SCAN: Run CLUSTER_HEALTH and FIND_ISSUES to identify all unhealthy resources.", "2. EVENT ANALYSIS: Run GET_EVENTS --all-namespaces to catch recent warnings.", "3. DEEP DIVE (Parallel): For every CrashLoopBackOff pod found, immediately run GET_LOGS --previous.", "4. DEEP DIVE (Parallel): For every Pending pod found, immediately run DESCRIBE to check events/scheduler.", "5. NODE CHECK: If Pending pods exist, check TOP_NODES or DESCRIBE NODE to verify capacity.", "6. PLATFORM CHECK: Run GET_CROSSPLANE and GET_ISTIO to verify platform components.", "7. KNOWLEDGE MATCH: For specific errors found in logs, run SEARCH_KNOWLEDGE with the error message.", "8. ADVANCED ANALYSIS: Use RUN_BASH for complex queries like filtering, sorting, or JSON extraction.", "9. SYNTHESIS: Correlate findings. Example: 'Pending Pods' + 'High CPU Node' = 'Cluster Full'."], "fixes": ["Follow the investigation flow to identify root causes. Use advanced tools when basic tools aren't providing enough insight."], "related_patterns": ["autonomy", "debugging", "playbook", "guide", "investigation"]}
{"id": "advanced-tools-guide", "category": "tools", "symptoms": [], "root_cause": "Guide for using advanced investigation tools like RUN_BASH, READ_FILE, FETCH_URL, and WEB_SEARCH for complex Kubernetes debugging scenarios.", "investigation": [], "fixes": [], "related_patterns": ["tools", "bash", "shell", "advanced", "power-user", "jq", "filtering", "mcp", "file", "url"]}
{"id": "kb-001", "category": "troubleshooting", "symptoms": [], "root_cause": "Troubleshooting guide for pods crashing due to missing Azure Workload Identity configuration", "investigation": [], "fixes": [], "related_patterns": ["azure", "authentication", "crashloopbackoff", "workload-identity"]}
{"id": "resource-not-found-placeholder-guidance", "category": "troubleshooting", "symptoms": ["Tool output says arguments contain placeholders", "Commands include [brackets], <angles>, or 'example-' prefixes", "Resource not found / unknown resource errors", "Investigation stops before running real tools", "PLACEHOLDER ERROR", "contains [brackets] or <angles>", "is not a real resource name", "Invalid argument .* placeholder", "NotFound", "not found", "unknown resource", "Placeholders or sample names used literally - Docs or LLM responses kept [pod-name] or example-app instead of real names.", "Wrong namespace or kind - Resource exists in another namespace or a different kind (Deployment vs Pod).", "Stale/terminated resources - Resource already deleted or replaced."], "root_cause": "How to recover when commands fail because of placeholders like [pod-name]/<namespace> or resource not found errors.", "investigation": [], "fixes": ["Run LIST_ALL (or FIND_ISSUES) to discover real names, then retry DESCRIBE/GET_LOGS/GET_EVENTS with the actual namespace/name.", "Discover real names via LIST_ALL/FIND_ISSUES; rerun DESCRIBE/GET_LOGS with actual ns/name.", "Use LIST_ALL <kind> and specify the correct namespace.", "Re-discover current names and target the latest pods/deployments."], "related_patterns": ["placeholder", "invalid-name", "not-found", "arguments", "kubectl", "tooling", "PLACEHOLDER ERROR", "contains [brackets] or <angles>", "is not a real resource name", "Invalid argument .* placeholder", "NotFound", "not found", "unknown resource"]}
{"id":"cluster-health-overview","category":"troubleshooting","symptoms":["Something is wrong in my cluster","Cluster is broken","Everything is failing","General health check","Is my cluster ok","Random issues across namespaces","I don't know what's wrong","find all issues","autonomous scan"],"root_cause":"High-level cluster health sweep for very vague or unknown issues. Runs a broad scan and then defers to more specific patterns (crashloop, scheduling, dns, resource, etc.).","investigation":["kubectl get nodes","kubectl get pods -A | head -n 50","kubectl get pods -A --field-selector=status.phase!=Running,status.phase!=Succeeded","kubectl get events -A --sort-by=.lastTimestamp | tail -n 50","kubectl top nodes || true","kubectl top pods -A | head -n 50 || true","1) Scan node health: NotReady/cordoned nodes → node/scheduling/resource patterns.","2) List non-Running pods across namespaces → route to crashloop/scheduling/imagepull/probe patterns.","3) Inspect recent Warning events → route to webhook, tls, quota, rbac, pvc, etc.","4) If many Pending pods → use scheduling-troubleshooting / resource-limits-troubleshooting.","5) If many CrashLoopBackOff/OOMKilled → use crashloop-troubleshooting / oomkilled-troubleshooting.","6) If widespread 4xx/5xx/DNS issues → use dns-networking-troubleshooting / istio-gateway-troubleshooting / service-connectivity-troubleshooting.","7) Summarize problems by namespace/component and propose a priority order."],"fixes":["Group findings by category (nodes, pods, networking, storage, autoscaling, platform controllers).","For each group, invoke the more specific playbook (crashloop, image-pull, scheduling, dns-networking, webhook, certificate-tls, resource-limits, etc.).","Apply targeted fixes and re-run the high-level health sweep to confirm improvements."],"related_patterns":["crashloop-troubleshooting","oomkilled-troubleshooting","scheduling-troubleshooting","dns-networking-troubleshooting","service-connectivity-troubleshooting","image-pull-troubleshooting","resource-limits-troubleshooting","webhook-troubleshooting","certificate-tls-troubleshooting","autonomous-playbook"]}
{"id":"generic-app-outage","category":"troubleshooting","symptoms":["My app is down","Service is not working","API is not responding","Website is down","Users are getting errors","Cannot reach service","Production is broken","cannot access app","fails in prod but works locally"],"root_cause":"Generic routing pattern for app or API outages when the user only names the app/service but not the error. It ties together ingress, service, endpoints, pods, DNS, and config issues.","investigation":["1) Identify the namespace and service name from user context or configuration.","kubectl get svc -A | grep -i <app-or-service-name>","kubectl get ingress -A | grep -i <app-or-service-name> || true","kubectl describe svc -n <ns> <svc>","kubectl get endpoints -n <ns> <svc> -o wide","kubectl get pods -n <ns> -l <svc-selector> -o wide","kubectl get pods -n <ns> -l <svc-selector> | grep -v Running || true","kubectl get events -n <ns> --sort-by=.lastTimestamp | tail -n 30","2) If no endpoints or empty endpoints → use service-connectivity-troubleshooting and crashloop-troubleshooting.","3) If pods Pending → use scheduling-troubleshooting / pvc/statefulset patterns.","4) If pods Ready but 4xx/5xx → use ingress/istio-gateway-troubleshooting / dns-networking-troubleshooting / probe-troubleshooting.","5) If only some clients see failures → check NetworkPolicies / mesh policies.","6) If config/auth-like errors in logs → route to configmap-secret-troubleshooting / rbac-permissions-troubleshooting / certificate-tls-troubleshooting."],"fixes":["Fix backing pods first (CrashLoop/OOM/Pending) using crashloop-troubleshooting, oomkilled-troubleshooting, scheduling-troubleshooting, statefulset-troubleshooting.","Align Service selector with pod labels; ensure endpoints are Ready.","Fix ingress/mesh routing (host/path/gateway/VirtualService) and DNS where applicable.","Resolve config/secret/auth issues seen in logs, then rollout and re-test."],"related_patterns":["service-connectivity-troubleshooting","dns-networking-troubleshooting","ingress-4xx-5xx-generic","istio-gateway-troubleshooting","crashloop-troubleshooting","image-pull-troubleshooting","configmap-secret-troubleshooting","rbac-permissions-troubleshooting","probe-troubleshooting"]}
{"id":"generic-performance-degradation","category":"troubleshooting","symptoms":["Cluster is slow","App is slow","High latency","Throughput dropped","Requests timing out intermittently","CPU is high","Memory is high","performance issue","sluggish","unresponsive under load"],"root_cause":"Generic performance routing pattern that connects resource pressure, throttling, HPA misconfig, DNS/mesh issues, and noisy neighbours.","investigation":["kubectl top nodes || true","kubectl top pods -A | head -n 50 || true","kubectl get hpa -A || true","kubectl get events -A --sort-by=.lastTimestamp | tail -n 50","1) Identify hot namespaces/deployments: sort pods by CPU/memory usage and error rates (if available from telemetry).","2) Check for OOMKilled/evictions → resource-limits-troubleshooting / oomkilled-troubleshooting.","3) If HPA present but replicas stuck → hpa-scaling-troubleshooting.","4) If latency spikes with 503/504/timeouts → service-connectivity-troubleshooting / dns-networking-troubleshooting / istio-gateway-troubleshooting.","5) Look for node pressure or uneven load across nodes.","6) For stateful systems (DB, queue) → check PVC health and statefulset-troubleshooting."],"fixes":["Right-size requests/limits for top offenders; avoid under-requesting workloads that hog nodes.","Adjust HPA targets and min/max replicas so scaling actually happens under load.","Add capacity or spread workloads across more nodes; mitigate noisy neighbours.","Fix DNS/mesh/NetworkPolicy issues causing retries/timeouts instead of real work.","For OOM or throttling, combine resource-limits-troubleshooting + oomkilled-troubleshooting + probe-troubleshooting."],"related_patterns":["resource-limits-troubleshooting","oomkilled-troubleshooting","hpa-scaling-troubleshooting","dns-networking-troubleshooting","service-connectivity-troubleshooting","istio-gateway-troubleshooting","statefulset-troubleshooting"]}
{"id":"cluster-access-troubleshooting","category":"troubleshooting","symptoms":["kubectl not working","cannot connect to cluster","i/o timeout connecting to API","certificate signed by unknown authority when using kubectl","context is wrong","forbidden when listing resources","connection refused to Kubernetes API","x509: certificate signed by unknown authority","You must be logged in to the server","Unauthorized","EOF from server"],"root_cause":"High-level pattern for problems talking to the cluster at all: kubeconfig, network, API server health, auth/RBAC, and TLS.","investigation":["1) Check current context:","kubectl config current-context || true","kubectl config get-contexts || true","2) Try simple call and capture error:","kubectl get ns || true","3) If auth/forbidden → use rbac-permissions-troubleshooting.","4) If x509/TLS → use certificate-tls-troubleshooting.","5) If timeout/connection refused → check control-plane health (if accessible) and network path:","  - ping / curl API endpoint from same host","  - check VPN/proxy and security groups/NSGs.","6) If multiple contexts exist → switch explicitly or ask user which cluster/kubeconfig to use.","7) For managed clusters (AKS/EKS/GKE) → verify cloud-side control plane status."],"fixes":["Fix kubeconfig (correct context, updated credentials or tokens).","Ensure network path to API server is reachable (VPN, firewall, DNS).","Resolve TLS trust issues with proper CA/certs for the API endpoint.","Adjust RBAC bindings so the user/ServiceAccount can list/get basic resources.","Once access is restored, hand over to cluster-health-overview or autonomous-playbook for deeper debugging."],"related_patterns":["rbac-permissions-troubleshooting","certificate-tls-troubleshooting","dns-networking-troubleshooting","kb-101"]}
{"id":"vague-issue-router","category":"playbook","symptoms":["It broke","Something is wrong","Not working","help me debug","I did something and now it fails","no idea what's going on","please investigate my cluster","god mode debug"],"root_cause":"Meta-router for extremely vague user input. It decides whether the problem is cluster-level, app-level, or access-level, then delegates.","investigation":["1) Try basic access: kubectl get ns || true.","   - If this fails → cluster-access-troubleshooting.","2) If access ok, run a light-weight health snapshot:","   - kubectl get nodes","   - kubectl get pods -A | head -n 30","   - kubectl get events -A --sort-by=.lastTimestamp | tail -n 30","3) If many namespaces affected → cluster-health-overview.","4) If mainly one app/namespace affected → generic-app-outage.","5) If symptoms mention 'slow', 'latency', 'timeout' → generic-performance-degradation.","6) Use SEARCH_KNOWLEDGE or internal pattern matching on events/log snippets to jump into a specific troubleshooting pattern.","7) Summarize what you think is broken and ask for minimal extra context only if absolutely necessary (e.g. 'which app/namespace is critical?')."],"fixes":["Always move from vague → concrete by collecting a health snapshot and mapping to existing specialized patterns.","Avoid guessing; prefer running CLUSTER_HEALTH / autonomous-playbook style scans when information is insufficient.","Once a culprit area is identified (pods, DNS, HPA, Crossplane, CAPI, UiPath, CastAI, etc.), fully switch to that specific playbook."],"related_patterns":["cluster-health-overview","generic-app-outage","generic-performance-degradation","cluster-access-troubleshooting","autonomous-playbook","placeholder-errors","resource-not-found-placeholder-guidance"]}
{"id":"resource-discovery-pods","category":"methodology","symptoms":["I only know part of the pod name","I don't remember the namespace","pod name contains","grep for pod","find the pod and debug it","find ns and pod from partial name"],"root_cause":"Standard, robust way for the agent to locate pods and namespaces from partial names, then pick the most relevant target and debug it. When ambiguity remains after ranking, the agent MUST ask the user which pod to investigate.","investigation":["1) Discover candidate pods across all namespaces using partial name (do NOT assume default namespace):","kubectl get pods -A | grep -i <partial-pod-name> || true","2) If no matches, try a wider search (labels, workloads):","kubectl get deploy,statefulset,daemonset -A | grep -i <partial-pod-name> || true","3) When multiple candidate pods exist, first rank them in this order:","   a) Non-Succeeded pods with unhealthy status (CrashLoopBackOff, Error, Pending, OOMKilled).","   b) Recently restarted pods (high RESTARTS, low AGE).","   c) Pods in namespaces the user mentioned or that are known as 'prod'/'stg'.","4) If a single candidate is clearly best (much more unhealthy / recent / matches ns), select that pod <ns>/<pod> automatically and proceed.","5) If SEVERAL candidates are still plausible (similar names or statuses) and confidence is low, the agent MUST ask a clarifying question, for example:","   - \"I found these pods matching '<partial>':\\n<ns1>/<pod1> (CrashLoopBackOff)\\n<ns2>/<pod2> (Running)\\nWhich one should I investigate?\"","   The agent should then WAIT for user choice before continuing.","6) Once a target <ns>/<pod> is agreed, perform deep-dive:","kubectl describe pod -n <ns> <pod>","kubectl logs -n <ns> <pod> --previous --tail=120 || kubectl logs -n <ns> <pod> --tail=120","7) If logs appear truncated or not enough context, expand progressively instead of spamming full logs:","   - kubectl logs -n <ns> <pod> --previous --tail=300","   - kubectl logs -n <ns> <pod> --previous --tail=1000","8) From describe/logs, map the error to a specific troubleshooting pattern (crashloop-troubleshooting, image-pull-troubleshooting, probe-troubleshooting, configmap-secret-troubleshooting, oomkilled-troubleshooting, etc.)."],"fixes":["Always start with cross-namespace discovery using grep on partial names instead of guessing a namespace.","Prefer pods that are clearly unhealthy and recently restarted over old, stable ones when multiple matches exist.","If multiple candidates are still plausible, explicitly show the list (ns, name, status) and ASK the user which one to inspect. Do NOT guess silently in that case.","Use a stepwise log expansion (tail 120 → 300 → 1000) and combine with 'kubectl describe pod' to get exit codes, events, and probe failures.","Once a clear error pattern is detected, switch to the corresponding troubleshooting entry and follow that playbook.","If the chosen pod looks healthy, explicitly tell the user and offer to inspect the next-best candidate instead of stopping silently."],"related_patterns":["crashloop-troubleshooting","image-pull-troubleshooting","probe-troubleshooting","oomkilled-troubleshooting","configmap-secret-troubleshooting","scheduling-troubleshooting","placeholder-errors","resource-not-found-placeholder-guidance","vague-issue-router"]}
{"id":"resource-discovery-generic","category":"methodology","symptoms":["find deployment by partial name","find service by partial name","not sure of namespace or exact name","resource not found when I run the command","unknown resource because of example names"],"root_cause":"Unified pattern for discovering the right resource (Deployment/Service/Ingress/etc.) from partial or fuzzy names, then avoiding placeholder/NotFound loops. When several matches remain, the agent asks the user which one to use.","investigation":["1) Never copy example names like [pod-name], <namespace>, example-app literally. Treat them as placeholders to replace.","2) For a generic name fragment <fragment>, search across common workload kinds and namespaces:","kubectl get deploy,statefulset,daemonset,job,cronjob,svc,ingress -A | grep -i <fragment> || true","3) If this yields many matches, prioritise:","   a) Names that most closely match the fragment (string similarity).","   b) Names in namespaces that are prod/stage or explicitly mentioned by the user.","   c) Objects with recent events or non-healthy status (Failed, CrashLoop, Pending, not Ready).","4) If a single resource is clearly the best candidate, select it and continue. If multiple are still plausible, the agent MUST show them clearly and ask a clarifying question, for example:","   - \"I found these resources matching '<fragment>':\\n1) deploy <ns1>/<name1> (Available=False)\\n2) svc <ns2>/<name2> (No endpoints)\\n3) ingress <ns3>/<name3> (Hosts: ...)\\nWhich one should I focus on first?\"","   The agent should WAIT for user selection before issuing describe/logs against one.","5) Once a specific <kind> <ns> <name> is selected (by ranking or by user choice), use that consistently in follow-up commands. Do NOT randomly switch to other namespaces unless explicitly changing target.","6) When a NotFound error occurs for a suspected resource:","   - Re-run the discovery step to refresh the list (resource might have been deleted or recreated).","   - If new candidates appear or the name changed, explain this and ask the user which to use.","7) Use LIST_ALL / FIND_ISSUES-style tools (if available) to quickly surface unhealthy resources and map them back to the partial names the user gave."],"fixes":["Default to discovery and ranking whenever the user input is fuzzy or incomplete instead of assuming default namespace or hard-coded names.","When multiple resources are plausible and confidence is low, interactively ask the user to choose from a concise list of candidates (kind/ns/name/status).","Refresh the resource list when NotFound appears; treat it as a signal that your internal view is stale and surface the updated options to the user.","Once you select a resource candidate, stay consistent with its <kind>/<ns>/<name> so logs, describe, and events all refer to the same object.","Map resulting errors (image pull, probe failure, RBAC, webhook, TLS, quota, PVC) to their dedicated troubleshooting patterns rather than inventing new ad-hoc logic."],"related_patterns":["placeholder-errors","resource-not-found-placeholder-guidance","cluster-health-overview","vague-issue-router","service-connectivity-troubleshooting","scheduling-troubleshooting","webhook-troubleshooting","certificate-tls-troubleshooting"]}
{"id":"namespace-vs-context-rule","category":"fundamental","symptoms":["model assumes namespace = context","model keeps using context as namespace","incorrect namespace inference"],"root_cause":"Kubernetes context and namespace are NOT the same but models mistakenly generalize them as equivalent.","investigation":["1) Check current context: kubectl config current-context","2) Check namespace explicitly assigned to that context: kubectl config view --minify --output 'jsonpath={..namespace}'","3) When namespace is unknown for a resource lookup, NEVER use context as namespace.","4) Instead perform namespace discovery: kubectl get pods -A | grep -i <name>"],"fixes":["Add explicit rule to prompts forbidding namespace inference from context.","Always treat context as cluster+user only.","Use -A discovery to find actual namespace of any resource."],"related_patterns":["resource-discovery-pods","resource-discovery-generic","namespaced-resource-discovery"]}
{"id":"namespaced-resource-discovery","category":"fundamental","symptoms":["find pod in cluster","find deployment","find service","locate resource","resource not found","which namespace","discover namespace","get resource details","kubectl get fails"],"root_cause":"CRITICAL: Logical process for discovering any namespaced Kubernetes resource. This is the MANDATORY flow an agent MUST follow to correctly locate resources.","investigation":["DECISION TREE FOR NAMESPACED RESOURCE DISCOVERY:","","STEP 1: DETERMINE WHAT YOU KNOW","├─ Know BOTH resource type AND exact name? → Go to STEP 2a","├─ Know resource type but NOT exact name? → Go to STEP 2b","├─ Know namespace but NOT resource type/name? → Go to STEP 2c","└─ Know NOTHING (vague query)? → Go to STEP 2d","","STEP 2a: KNOWN TYPE + NAME (e.g., 'get pod nginx-abc123')","  1) First try with -A to find namespace: kubectl get <type> -A | grep -i <name>","  2) Once namespace found: kubectl get <type> <name> -n <namespace>","  3) For details: kubectl describe <type> <name> -n <namespace>","  NEVER assume 'default' namespace - always discover first!","","STEP 2b: KNOWN TYPE, UNKNOWN NAME (e.g., 'find failing pods')","  1) List all of that type: kubectl get <type> -A","  2) Filter by status if needed: kubectl get <type> -A | grep -v Running","  3) Or filter by label: kubectl get <type> -A -l app=myapp","  4) Once specific resource found, use describe for details","","STEP 2c: KNOWN NAMESPACE, UNKNOWN RESOURCE (e.g., 'what's in namespace X')","  1) List common resources: kubectl get all -n <namespace>","  2) Note: 'get all' only shows: pods, services, deployments, replicasets, statefulsets, daemonsets, jobs, cronjobs","  3) For CRDs/custom resources, explicitly query: kubectl get <crd-type> -n <namespace>","  4) For full inventory: kubectl api-resources --namespaced=true | while read r; do kubectl get $r -n <ns>; done","","STEP 2d: VAGUE QUERY (e.g., 'something is wrong')","  1) Start with cluster-wide unhealthy scan: kubectl get pods -A --field-selector=status.phase!=Running,status.phase!=Succeeded","  2) Check recent events: kubectl get events -A --sort-by=.lastTimestamp | tail -50","  3) From results, identify namespace and resource, then proceed to specific investigation","","KEY RULES:","- NEVER guess namespace. ALWAYS use -A (all namespaces) first when namespace unknown","- NEVER use kubectl context as namespace - they are different concepts","- 'default' namespace is NOT a safe assumption - many clusters don't use it","- For partial names, use grep: kubectl get <type> -A | grep -i <partial>","- Namespaced resources: pods, deployments, services, configmaps, secrets, pvc, ingress, etc.","- Cluster-scoped resources (no namespace): nodes, namespaces, clusterroles, storageclasses, pvs"],"fixes":["When resource not found, ALWAYS re-run discovery with -A flag","When namespace unknown, discover it first before running describe/logs","Store discovered namespace and reuse it for subsequent commands on same resource","If partial name given, use grep -i for case-insensitive matching across all namespaces"],"related_patterns":["resource-discovery-pods","resource-discovery-generic","namespace-vs-context-rule","cluster-health-overview","vague-issue-router"]}
{"id":"crossplane-managed-resource-discovery","category":"fundamental","symptoms":["find sql server","find database","find storage account","find redis","find managed resource","crossplane resource health","azure resource status","aws resource status","gcp resource status","cloud resource not found","managed resource failing","crossplane synced false","crossplane ready false","find bucket","find vpc","find rds","find cosmos","find keyvault","cloud infrastructure status"],"root_cause":"CRITICAL: Universal pattern for discovering ANY Crossplane-managed cloud resource. Crossplane manages cloud infrastructure (databases, storage, networking, etc.) as Kubernetes CRDs. This is the MANDATORY discovery flow.","investigation":["CROSSPLANE MANAGED RESOURCE DISCOVERY FLOW:","","STEP 1: DISCOVER WHAT CROSSPLANE MANAGES IN THIS CLUSTER","  kubectl api-resources | grep -E 'crossplane|upbound|aws|azure|gcp' | head -50","  This shows all CRD types available (e.g., servers.dbforpostgresql.azure.upbound.io)","","STEP 2: UNIVERSAL MANAGED RESOURCE QUERY","  kubectl get managed","  This lists ALL Crossplane-managed resources regardless of type (SQL, storage, network, etc.)","  Output shows: NAME, SYNCED, READY, EXTERNAL-NAME, AGE","","STEP 3: FILTER BY KEYWORD (for specific resource types)","  kubectl get managed | grep -i <keyword>","  Common keywords: sql, postgres, mysql, redis, storage, bucket, vpc, subnet, cosmos, keyvault, rds","  Example: kubectl get managed | grep -i sql","","STEP 4: FIND UNHEALTHY RESOURCES","  kubectl get managed | grep -E 'False|Unknown'","  Resources with SYNCED=False or READY=False need investigation","","STEP 5: GET DETAILS FOR SPECIFIC RESOURCE","  Once you find the resource name and type from Step 2-4:","  kubectl describe <full-crd-type> <resource-name>","  Example: kubectl describe servers.dbforpostgresql.azure.upbound.io my-postgres-server","","  Key fields to check in describe output:","  - status.conditions[].type: Synced, Ready, LastAsyncOperation","  - status.conditions[].status: True/False/Unknown","  - status.conditions[].reason: Why it failed","  - status.conditions[].message: Detailed error","  - status.atProvider: Actual cloud state (if synced)","","STEP 6: CHECK PROVIDER HEALTH","  kubectl get providers","  kubectl describe provider <provider-name>","  Unhealthy provider = all its managed resources will fail","","COMMON CROSSPLANE CRD PATTERNS BY CLOUD:","","AZURE (upbound provider):","  kubectl get servers.dbforpostgresql.azure.upbound.io     # PostgreSQL","  kubectl get servers.dbformysql.azure.upbound.io          # MySQL","  kubectl get servers.sql.azure.upbound.io                 # SQL Server","  kubectl get accounts.storage.azure.upbound.io            # Storage Account","  kubectl get vaults.keyvault.azure.upbound.io             # Key Vault","  kubectl get accounts.cosmosdb.azure.upbound.io           # CosmosDB","  kubectl get caches.redis.azure.upbound.io                # Redis Cache","","AWS (upbound provider):","  kubectl get instances.rds.aws.upbound.io                 # RDS instances","  kubectl get clusters.rds.aws.upbound.io                  # RDS Aurora clusters","  kubectl get buckets.s3.aws.upbound.io                    # S3 buckets","  kubectl get vpcs.ec2.aws.upbound.io                      # VPCs","  kubectl get clusters.elasticache.aws.upbound.io          # ElastiCache","","GCP (upbound provider):","  kubectl get instances.sql.gcp.upbound.io                 # Cloud SQL","  kubectl get buckets.storage.gcp.upbound.io               # GCS buckets","  kubectl get instances.redis.gcp.upbound.io               # Memorystore Redis","","HEALTH CHECK INDICATORS:","  SYNCED=True + READY=True  → Resource is healthy and matches desired state","  SYNCED=True + READY=False → Resource exists but not fully provisioned","  SYNCED=False + READY=*    → Crossplane cannot sync to cloud (auth/quota/API error)","","QUICK HEALTH SCAN:","  kubectl get managed -o custom-columns='NAME:.metadata.name,KIND:.kind,SYNCED:.status.conditions[?(@.type==\"Synced\")].status,READY:.status.conditions[?(@.type==\"Ready\")].status'"],"fixes":["Always start with 'kubectl get managed' to discover what exists","Never guess CRD names - use 'kubectl api-resources | grep' to find exact types","When resource not found, check if provider is healthy first","For unhealthy resources, always check status.conditions for root cause","Use 'kubectl get managed | grep -i <keyword>' for fuzzy search across all types","Remember: Crossplane resources are cluster-scoped (no namespace needed)"],"related_patterns":["crossplane-troubleshooting","crossplane-provider-health","namespaced-resource-discovery","resource-discovery-generic","vague-issue-router"]}
