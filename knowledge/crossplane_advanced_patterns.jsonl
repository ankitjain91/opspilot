{"id":"crossplane-managed-resource-synced-false","category":"troubleshooting","symptoms":["Managed resource shows SYNCED=False","Resource stuck in provisioning","status.conditions shows False"],"root_cause":"Crossplane cannot sync the resource state with the cloud provider, usually due to API errors, credential issues, or the resource not existing in the provider","investigation":["kubectl get managed -A | grep -v True to find unsynced resources","kubectl describe <managed-resource> -n <namespace> to see status.conditions","Check status.message for provider API error details","Verify ProviderConfig credentials with kubectl get providerconfig","Check provider pod logs: kubectl logs -n crossplane-system -l pkg.crossplane.io/provider=<provider>"],"fixes":["If credential issue: Update ProviderConfig secret","If resource deleted manually in cloud: Delete the managed resource and recreate","If API rate limit: Wait and resource will retry","If provider bug: Check Crossplane provider GitHub issues and upgrade"],"related_patterns":["crossplane-provider-pod-crashloop","azure-rbac-permission-denied"]}
{"id":"crossplane-claim-waiting-for-composite","category":"troubleshooting","symptoms":["Claim shows 'Waiting for composite resource to become Ready'","Claim stuck in Pending state","No composite resource created"],"root_cause":"The Composition or CompositeResourceDefinition (XRD) is misconfigured, preventing the composite from being created","investigation":["kubectl get claim <name> -n <namespace> -o yaml | grep -A5 status to see claim status","kubectl get composite to check if composite was created","kubectl get composition to verify composition exists","Check XRD exists: kubectl get xrd","Review composition patches and resource references for syntax errors"],"fixes":["Verify Composition references correct XRD in spec.compositeTypeRef","Check all required fields in Composition spec.resources are valid","Ensure CompositeResourceDefinition has matching spec.names","If patches are wrong: Fix composition patches syntax","Delete and recreate claim if composite is orphaned"],"related_patterns":["crossplane-composition-patch-error","crossplane-xrd-validation-error"]}
{"id":"crossplane-composition-patch-error","category":"troubleshooting","symptoms":["Composite resource created but managed resources missing","status.conditions shows 'cannot compose resources'","Patch transformation errors in logs"],"root_cause":"Composition patches have invalid syntax, reference non-existent fields, or use unsupported transform types","investigation":["kubectl describe composite <name> to see patch errors","Check composition spec.resources[].patches for syntax","Verify fromFieldPath exists in claim/composite spec","Test patch transforms with example values","Review provider logs: kubectl logs -n crossplane-system -l pkg.crossplane.io/provider"],"fixes":["Fix patch field paths - ensure they match actual resource schema","Use correct transform types: map, string, math, convert","Add default values for optional fields using policy: fromFieldPath: Optional","Validate composition with: kubectl apply --dry-run=server","Simplify complex patches - break into multiple steps"],"related_patterns":["crossplane-claim-waiting-for-composite","crossplane-provider-schema-mismatch"]}
{"id":"crossplane-provider-pod-crashloop","category":"troubleshooting","symptoms":["Provider pod in CrashLoopBackOff","Provider shows Healthy=False","Managed resources not reconciling"],"root_cause":"Provider failed to start due to invalid configuration, credential issues, OOM, or provider image pull errors","investigation":["kubectl get providers to see provider health","kubectl logs -n crossplane-system deployment/<provider> --previous to see crash logs","Check provider config: kubectl get providerconfig -o yaml","Check pod resource limits: kubectl describe pod -n crossplane-system -l pkg.crossplane.io/provider","Look for OOMKilled: kubectl get pods -n crossplane-system -o jsonpath='{.items[*].status.containerStatuses[*].lastState.terminated.reason}'"],"fixes":["If OOMKilled: Increase provider pod memory limits in provider deployment","If credential error: Fix ProviderConfig secret - ensure service principal/managed identity is valid","If image pull error: Check provider version and registry access","If API compatibility: Upgrade/downgrade provider version","Delete provider pod to force restart: kubectl delete pod -n crossplane-system -l pkg.crossplane.io/provider"],"related_patterns":["azure-service-principal-expired","crossplane-managed-resource-synced-false"]}
{"id":"crossplane-azure-roleassignment-403","category":"troubleshooting","symptoms":["RoleAssignment managed resource shows error 403","status.message: 'The client does not have authorization'","SYNCED=False on RoleAssignment"],"root_cause":"The service principal or managed identity used by Crossplane provider-azure lacks sufficient RBAC permissions to create role assignments","investigation":["kubectl describe roleassignment <name> -n <namespace> to see exact error","Check which ProviderConfig is being used","Get service principal/managed identity from ProviderConfig secret","In Azure Portal: Check role assignments for the identity","Verify identity has 'User Access Administrator' or 'Owner' role at appropriate scope"],"fixes":["Grant the Crossplane identity 'User Access Administrator' role: az role assignment create --assignee <sp-object-id> --role 'User Access Administrator' --scope <scope>","Or use 'Owner' role if needed (more permissive)","Ensure role is assigned at the correct scope (subscription/resource group)","Wait 5-10 minutes for Azure RBAC to propagate","Verify with: az role assignment list --assignee <sp-object-id>"],"related_patterns":["azure-rbac-permission-denied","crossplane-managed-resource-synced-false","azure-service-principal-expired"]}
{"id":"crossplane-azure-resourcegroup-deletion-blocked","category":"troubleshooting","symptoms":["ResourceGroup deletion stuck","Azure shows 'Resource group contains resources'","Managed resource stuck in deleting state for >15 minutes"],"root_cause":"Azure Resource Group cannot be deleted because it still contains child resources that Crossplane didn't clean up, or manual resources were added","investigation":["Check Azure Portal for resources still in the resource group","kubectl describe resourcegroup <name> to see deletion status","Check for finalizers: kubectl get resourcegroup <name> -o yaml | grep finalizers","List all managed resources in that RG: kubectl get managed -A | grep <rg-name>","Check for orphaned Azure resources: az resource list --resource-group <rg-name>"],"fixes":["Delete child resources first: kubectl delete managed --all -n <namespace>","If manual resources exist in Azure: Delete them via portal or CLI","Remove finalizer if stuck: kubectl patch resourcegroup <name> -p '{\"metadata\":{\"finalizers\":null}}' --type=merge","Force delete the resource group in Azure: az group delete --name <rg> --yes --no-wait","Re-sync Crossplane state: Delete the managed resource object"],"related_patterns":["crossplane-managed-resource-stuck-deleting","azure-resource-locked"]}
{"id":"crossplane-provider-config-not-found","category":"troubleshooting","symptoms":["Managed resource shows 'cannot get referenced ProviderConfig'","Composite resources not creating managed resources","Provider logs show ProviderConfig lookup errors"],"root_cause":"The ProviderConfig referenced by the managed resource doesn't exist, was deleted, or is in a different namespace (ProviderConfigs are cluster-scoped)","investigation":["kubectl get providerconfig to list all configs","Check managed resource spec.providerConfigRef.name","Verify the name matches exactly (case-sensitive)","Check if default ProviderConfig exists: kubectl get providerconfig default","Review namespace - ProviderConfigs are cluster-scoped, not namespaced"],"fixes":["Create the missing ProviderConfig","Or update managed resource to reference existing config","Create default ProviderConfig if using default: kubectl apply -f providerconfig.yaml","Check composition - ensure it sets correct providerConfigRef","If using patches: Verify patch correctly sets providerConfigRef"],"related_patterns":["crossplane-provider-pod-crashloop","azure-service-principal-expired"]}
{"id":"vcluster-api-server-not-ready","category":"troubleshooting","symptoms":["vcluster pod shows 0/1 Ready","Cannot kubectl into vcluster","Connection refused when using vcluster connect"],"root_cause":"vcluster control plane components (API server, etcd, controller manager) failed to start, usually due to resource constraints, certificate issues, or volume mounting errors","investigation":["kubectl logs <vcluster-pod> -c syncer to check syncer logs","kubectl logs <vcluster-pod> -c vcluster to check control plane logs","kubectl describe pod <vcluster-pod> to see events","Check pod resource usage: kubectl top pod <vcluster-pod>","Verify persistent volume: kubectl get pvc -n <vcluster-namespace>"],"fixes":["If OOMKilled: Increase vcluster pod memory limits in values.yaml","If PVC pending: Check storage class and provisioner","If certificate error: Delete vcluster pod to regenerate certs","If etcd corruption: Delete PVC and recreate vcluster (data loss!)","Check vcluster logs for specific error and search vcluster GitHub issues"],"related_patterns":["vcluster-sync-failing","etcd-database-corruption"]}
{"id":"vcluster-sync-failing","category":"troubleshooting","symptoms":["Resources created in vcluster not appearing in host cluster","vcluster syncer logs show sync errors","Pods in vcluster stuck in Pending"],"root_cause":"vcluster syncer failed to sync resources between virtual and host cluster due to RBAC issues, label selector mismatches, or resource conflicts","investigation":["kubectl logs <vcluster-pod> -c syncer | grep -i error","Check syncer RBAC: kubectl get clusterrole vcluster-<name>","Verify sync configuration in vcluster values.yaml","Check for conflicting resources in host cluster with same name","Test creating a simple pod in vcluster: vcluster connect <name> -- kubectl run test --image=nginx"],"fixes":["Update vcluster RBAC if missing permissions","Check sync.toHost and sync.fromHost configuration","Ensure no resource name conflicts between vcluster and host","Restart syncer: kubectl delete pod <vcluster-pod>","Review vcluster values for correct sync labels and selectors"],"related_patterns":["vcluster-api-server-not-ready","rbac-permission-denied"]}
{"id":"vcluster-dns-resolution-failed","category":"troubleshooting","symptoms":["Pods in vcluster cannot resolve service names","nslookup fails inside vcluster pods","Connection timeouts to services"],"root_cause":"CoreDNS in vcluster is not properly configured, or host cluster DNS is not accessible from vcluster pods","investigation":["Test DNS from vcluster pod: vcluster connect <name> -- kubectl run -it dns-test --image=busybox --rm -- nslookup kubernetes.default","Check CoreDNS pods in vcluster: vcluster connect <name> -- kubectl get pods -n kube-system -l k8s-app=kube-dns","Review CoreDNS config: vcluster connect <name> -- kubectl get cm -n kube-system coredns -o yaml","Check if host cluster DNS is reachable: kubectl exec <vcluster-pod> -c vcluster -- nslookup google.com"],"fixes":["Restart CoreDNS in vcluster: vcluster connect <name> -- kubectl delete pod -n kube-system -l k8s-app=kube-dns","Update CoreDNS config to use host DNS as upstream","Check vcluster values.yaml for correct DNS settings","Ensure host cluster allows DNS traffic from vcluster pods","Verify network policies not blocking DNS (port 53)"],"related_patterns":["coredns-crashloop","network-policy-blocking-dns","vcluster-api-server-not-ready"]}
{"id":"custom-crd-reconciliation-loop","category":"troubleshooting","symptoms":["Controller pod logs show repeated reconcile attempts","Custom resource status not updating","Controller logs flooding with same resource"],"root_cause":"Controller reconciliation logic has a bug causing infinite loop, or status update is failing causing requeue, or watch mechanism is broken","investigation":["kubectl logs -n <controller-namespace> <controller-pod> | grep -A10 'Reconciling' to see reconcile frequency","Check if status subresource is enabled in CRD: kubectl get crd <crd-name> -o yaml | grep subresources","Monitor reconcile rate: watch 'kubectl get <custom-resource> <name> -o yaml | grep resourceVersion'","Check controller metrics for reconcile queue depth","Review controller code for missing return statements or error handling"],"fixes":["Add proper error handling in controller to avoid requeue on permanent errors","Ensure status updates use status subresource: UpdateStatus() not Update()","Implement exponential backoff for failed reconciles","Add requeueAfter with delay instead of immediate requeue","Check for missing RBAC permissions causing status update failures"],"related_patterns":["controller-pod-high-cpu","custom-crd-status-not-updating"]}
{"id":"custom-crd-status-not-updating","category":"troubleshooting","symptoms":["Custom resource status field remains empty","kubectl get shows no status columns","Controller successfully reconciles but status doesn't change"],"root_cause":"CRD doesn't have status subresource enabled, or controller lacks RBAC to update status, or controller is updating spec instead of status","investigation":["Check CRD definition: kubectl get crd <crd-name> -o yaml | grep -A5 subresources","Verify status subresource enabled: should see 'status: {}'","Check controller RBAC: kubectl get clusterrole <controller-role> -o yaml | grep status","Review controller logs for status update errors","Test manual status update: kubectl patch <resource> --subresource=status"],"fixes":["Enable status subresource in CRD spec.versions[].subresources.status: {}","Add RBAC rule: {apiGroups: [...], resources: [..., .../status], verbs: [update, patch]}","Update controller code to use r.Status().Update() not r.Update()","Ensure CRD changes are applied: kubectl apply -f crd.yaml","Restart controller after CRD/RBAC changes"],"related_patterns":["custom-crd-reconciliation-loop","rbac-permission-denied"]}
{"id":"controller-webhook-timeout","category":"troubleshooting","symptoms":["Resource creation times out","Error: 'context deadline exceeded' when creating resources","Webhook admission errors in API server logs"],"root_cause":"Mutating or validating webhook is not responding, controller webhook pod is down, or webhook service is misconfigured","investigation":["Check webhook configuration: kubectl get validatingwebhookconfigurations,mutatingwebhookconfigurations","Verify webhook pod is running: kubectl get pods -n <controller-namespace> -l control-plane=controller-manager","Test webhook service: kubectl get svc -n <controller-namespace>","Check webhook pod logs: kubectl logs -n <controller-namespace> <webhook-pod>","Verify webhook cert is valid: kubectl get secret -n <controller-namespace> webhook-server-cert -o yaml"],"fixes":["If webhook pod down: Check pod events and restart","If service misconfigured: Verify service selector matches pod labels","If cert expired: Regenerate webhook certificates using cert-manager or manual process","Temporarily remove webhook to unblock: kubectl delete validatingwebhookconfiguration <name>","Check webhook failurePolicy - set to Ignore for non-critical webhooks"],"related_patterns":["cert-manager-certificate-not-ready","api-server-webhook-latency"]}
{"id":"azure-managed-identity-not-found","category":"troubleshooting","symptoms":["Azure API returns 'identity not found'","Crossplane managed resources fail with identity errors","ProviderConfig shows authentication failures"],"root_cause":"Managed identity assigned to AKS/VM doesn't exist, not assigned to the node pool, or hasn't propagated to Azure AD","investigation":["Check AKS managed identity: az aks show -n <cluster> -g <rg> --query identityProfile","List identities on node: az vm identity show -n <node> -g <node-rg>","Verify identity exists: az identity show -n <identity-name> -g <rg>","Check ProviderConfig uses correct identity: kubectl get providerconfig -o yaml","Check Azure AD propagation time (can take 5-10 minutes)"],"fixes":["Assign managed identity to AKS: az aks update -n <cluster> -g <rg> --assign-identity <identity-id>","Or assign to VMSS: az vmss identity assign -n <vmss> -g <rg> --identities <identity-id>","Wait 5-10 minutes for Azure AD propagation","Verify identity has required role assignments","Update ProviderConfig to use correct clientId or remove for system-assigned"],"related_patterns":["crossplane-azure-roleassignment-403","azure-service-principal-expired"]}
{"id":"azure-service-principal-expired","category":"troubleshooting","symptoms":["Crossplane Azure resources fail with 'AADSTS700016: Application not found'","ProviderConfig auth fails","All Azure managed resources show SYNCED=False"],"root_cause":"Service principal secret expired, service principal was deleted, or credentials in Kubernetes secret are wrong","investigation":["Check SP expiration: az ad sp credential list --id <app-id> | jq '.[].endDateTime'","Verify SP exists: az ad sp show --id <app-id>","Check Kubernetes secret: kubectl get secret -n crossplane-system <provider-secret> -o yaml | base64 -d","Test credentials manually: az login --service-principal -u <app-id> -p <secret> --tenant <tenant>","Review ProviderConfig: kubectl get providerconfig -o yaml"],"fixes":["Generate new secret: az ad sp credential reset --id <app-id>","Update Kubernetes secret with new credentials","Or create new service principal: az ad sp create-for-rbac -n <name> --role Contributor","Restart provider pod: kubectl delete pod -n crossplane-system -l pkg.crossplane.io/provider=azure","Consider using managed identity instead of service principal"],"related_patterns":["crossplane-provider-pod-crashloop","azure-rbac-permission-denied"]}
{"id":"azure-resource-locked","category":"troubleshooting","symptoms":["Cannot delete Azure resource","Error: 'resource is locked'","Crossplane deletion stuck"],"root_cause":"Azure resource has a delete lock applied via Azure Policy or manual lock","investigation":["Check for locks in Azure Portal on the resource","Or via CLI: az lock list --resource-group <rg>","Check resource-level locks: az lock list --resource <resource-id>","Review Azure Policy assignments","Verify lock inheritance from resource group or subscription"],"fixes":["Remove lock: az lock delete --name <lock-name> --resource-group <rg>","Or delete resource-level lock: az lock delete --name <lock-name> --resource <resource-id>","Check Azure Policy - may need to add exception","After removing lock, retry Crossplane deletion","If policy re-applies lock: Contact Azure admin to modify policy"],"related_patterns":["crossplane-azure-resourcegroup-deletion-blocked","azure-rbac-permission-denied"]}
{"id":"crossplane-provider-upgrade-breaking-change","category":"troubleshooting","symptoms":["Managed resources stop reconciling after provider upgrade","New API version not supported","Status shows 'cannot parse resource'"],"root_cause":"Provider upgrade introduced breaking API changes or dropped support for old API versions","investigation":["Check provider release notes: https://github.com/crossplane-contrib/provider-<provider>/releases","Compare old vs new provider version: kubectl get providers","Review managed resource API versions: kubectl get managed -A -o yaml | grep apiVersion","Check provider logs for deprecation warnings: kubectl logs -n crossplane-system -l pkg.crossplane.io/provider","Verify composition uses supported API versions"],"fixes":["Downgrade provider temporarily: kubectl edit provider <provider> and change spec.package version","Update managed resource API versions to new format","Update compositions to use new resource schemas","Review migration guide in provider documentation","Test in non-prod environment before upgrading prod"],"related_patterns":["crossplane-composition-patch-error","kubernetes-api-deprecation"]}
{"id":"crossplane-composition-missing-required-field","category":"troubleshooting","symptoms":["Composite resource shows 'cannot compose resources: field required'","Managed resources not created","Composition validation errors"],"root_cause":"Composition is missing required fields for managed resources, or patches don't provide required values","investigation":["kubectl describe composite <name> to see exact missing field","Review composition spec.resources for the failing resource","Check provider CRD schema: kubectl get crd <managed-resource-crd> -o yaml","Identify required fields in spec (required: true)","Verify patches provide values for all required fields"],"fixes":["Add missing required fields to composition spec.resources[].base","Or add patch to populate field from claim: {fromFieldPath: ..., toFieldPath: ...}","Set default values for optional fields using: policy: fromFieldPath: Optional","Validate composition: kubectl apply --dry-run=server -f composition.yaml","Update composition and delete/recreate composite to retry"],"related_patterns":["crossplane-composition-patch-error","crossplane-claim-waiting-for-composite"]}
{"id":"controller-cache-out-of-sync","category":"troubleshooting","symptoms":["Controller acts on stale resource state","Status shows old values","Controller logs 'resource not found' for existing resources"],"root_cause":"Controller's informer cache is out of sync with API server due to watch connection drops or cache initialization failures","investigation":["Check controller logs for 'watch closed' or 'cache sync' errors","Verify API server connectivity: kubectl logs -n <controller-ns> <controller-pod> | grep connection","Monitor watch connections: kubectl get --raw /metrics | grep watch","Check controller has proper RBAC for list/watch: kubectl get clusterrole <role> -o yaml","Test if manual pod restart fixes it"],"fixes":["Restart controller pod: kubectl delete pod -n <controller-ns> <controller-pod>","Increase controller cache sync timeout in code","Check for network policies blocking API server access","Verify controller service account has list/watch permissions","Review controller-runtime version - upgrade if on old version"],"related_patterns":["api-server-connection-refused","rbac-permission-denied"]}
{"id":"crossplane-provider-rate-limit","category":"troubleshooting","symptoms":["Provider logs show 429 rate limit errors","Random reconciliation failures","Azure/AWS API throttling messages"],"root_cause":"Crossplane making too many API calls to cloud provider causing rate limiting","investigation":["Check provider logs: kubectl logs -n crossplane-system -l pkg.crossplane.io/provider | grep 429","Count managed resources: kubectl get managed --all-namespaces | wc -l","Check reconciliation frequency in provider config","Review cloud provider rate limits documentation","Monitor API call metrics in cloud provider console"],"fixes":["Reduce managed resource count or spread across multiple provider instances","Increase poll interval: Update provider config reconcileInterval","Implement exponential backoff in compositions","Use provider features like connection pooling","Request higher API rate limits from cloud provider","Consider using multiple ProviderConfigs with different credentials"],"related_patterns":["crossplane-managed-resource-synced-false","azure-api-throttling"]}
{"id":"vcluster-persistent-volume-sync-issue","category":"troubleshooting","symptoms":["PVCs in vcluster stuck in Pending","Host cluster shows PVC but no PV created","Storage class mismatch between vcluster and host"],"root_cause":"vcluster not properly syncing PVCs to host cluster, or host storage class doesn't match vcluster storage class","investigation":["Check syncer logs: kubectl logs <vcluster-pod> -c syncer | grep PersistentVolumeClaim","Verify PVC sync enabled: Check vcluster values.yaml sync.toHost.persistentVolumeClaims","List PVCs in host: kubectl get pvc -n <vcluster-namespace>","Check storage class exists in host: kubectl get sc","Compare storage classes: vcluster connect <name> -- kubectl get sc vs kubectl get sc"],"fixes":["Enable PVC sync in vcluster values: sync.toHost.persistentVolumeClaims.enabled: true","Map storage classes: sync.toHost.persistentVolumeClaims.rewriteStorageClass: <host-sc>","Or use same storage class name in both clusters","Recreate vcluster with correct sync configuration","Manually create PVC in host cluster as workaround"],"related_patterns":["vcluster-sync-failing","persistent-volume-claim-pending"]}
{"id":"custom-controller-leader-election-failing","category":"troubleshooting","symptoms":["Multiple controller instances active simultaneously","Controller logs show 'failed to acquire lease'","Duplicate reconciliation of same resource"],"root_cause":"Leader election failing due to RBAC issues, configmap/lease resource access problems, or clock skew","investigation":["Check controller logs: kubectl logs -n <controller-ns> -l control-plane=controller-manager | grep lease","Verify leader election ConfigMap/Lease: kubectl get lease -n <controller-ns>","Check RBAC for leases: kubectl get role/clusterrole <controller-role> -o yaml | grep leases","Verify only one controller pod should be leader","Check node clock skew: date on each node"],"fixes":["Add lease RBAC: {apiGroups: ['coordination.k8s.io'], resources: ['leases'], verbs: ['get','create','update']}","Check controller flags: ensure --leader-elect=true and --leader-election-id is unique","Delete stale lease: kubectl delete lease -n <controller-ns> <lease-name>","Ensure controller uses same lease namespace and name","Fix node clock skew using NTP"],"related_patterns":["custom-crd-reconciliation-loop","rbac-permission-denied"]}
{"id":"crossplane-composition-dependency-ordering","category":"troubleshooting","symptoms":["Managed resources created in wrong order","Child resources fail because parent doesn't exist yet","Race condition in resource creation"],"root_cause":"Composition doesn't define proper resource dependencies, causing resources to be created in parallel when they need sequential creation","investigation":["Review composition spec.resources order (created top to bottom)","Check for cross-resource references (e.g., subnet needs VNet first)","Look for 'resource not found' errors in provider logs","Identify which resource is the dependency","Test creation order manually"],"fixes":["Reorder spec.resources in composition - dependencies first","Use readinessChecks to wait for parent resource: spec.resources[].readinessChecks","Add explicit dependencies using Crossplane functions (composition functions)","Split into multiple compositions with claim ordering","Use patches to pass resource references only after parent is ready"],"related_patterns":["crossplane-composition-patch-error","crossplane-managed-resource-synced-false"]}
{"id":"controller-high-memory-leak","category":"troubleshooting","symptoms":["Controller pod memory usage constantly increasing","Eventually OOMKilled","Memory doesn't decrease after deleting resources"],"root_cause":"Controller has memory leak due to unclosed clients, cached objects not being garbage collected, or goroutine leaks","investigation":["Monitor memory over time: kubectl top pod -n <controller-ns> <controller-pod> --containers","Take heap dump: kubectl exec -n <controller-ns> <controller-pod> -- curl localhost:8080/debug/pprof/heap > heap.prof","Check goroutine count: kubectl exec <controller-pod> -- curl localhost:8080/debug/pprof/goroutine?debug=1","Review controller code for defer client.Close() or context cancellation","Count watches: kubectl get --raw /metrics | grep apiserver_watch_events_total"],"fixes":["Increase memory limits temporarily: kubectl set resources deployment <controller> --limits=memory=2Gi","Restart controller periodically as workaround: Add liveness probe with memory threshold","Fix code: Ensure contexts are cancelled, clients closed, cache has size limits","Upgrade controller-runtime and dependencies","Profile with pprof and fix leaking goroutines"],"related_patterns":["controller-pod-high-cpu","oomkilled-container"]}
{"id":"vcluster-ingress-not-working","category":"troubleshooting","symptoms":["Ingress created in vcluster doesn't route traffic","Ingress controller in host cluster doesn't see vcluster ingress","404 errors when accessing vcluster ingress URLs"],"root_cause":"vcluster ingress sync not enabled, or ingress class mismatch between vcluster and host","investigation":["Check if ingress sync enabled: vcluster values sync.toHost.ingresses.enabled","Verify ingress created in vcluster: vcluster connect <name> -- kubectl get ingress","Check host cluster for synced ingress: kubectl get ingress -n <vcluster-namespace>","Compare ingress classes: vcluster connect <name> -- kubectl get ingressclass vs kubectl get ingressclass","Check syncer logs for ingress sync errors"],"fixes":["Enable ingress sync in vcluster values: sync.toHost.ingresses.enabled: true","Set ingress class translation: sync.toHost.ingresses.translateIngressClass","Ensure host ingress controller is running: kubectl get pods -n ingress-nginx","Update vcluster ingress to use host's ingress class","Restart vcluster pod after values change"],"related_patterns":["vcluster-sync-failing","ingress-controller-not-routing"]}
{"id":"crossplane-azure-aks-creation-timeout","category":"troubleshooting","symptoms":["AKS cluster managed resource stuck in creating","Timeout after 30+ minutes","Azure portal shows cluster creating"],"root_cause":"Azure AKS cluster creation is slow (10-30 minutes normal), or Crossplane timeout is too short, or actual Azure provisioning failure","investigation":["Check Azure portal - is cluster actually creating?","Review managed resource status: kubectl describe akscluster <name> -n <namespace>","Check Azure activity log for errors: az monitor activity-log list --resource-group <rg>","Verify all prerequisites exist: VNet, subnet, service principal","Check status.message for specific Azure error"],"fixes":["Increase Crossplane timeout if needed (default is 60 minutes)","Check Azure subscription quota for AKS: az vm list-usage --location <region>","Verify VNet/subnet CIDR ranges don't overlap with cluster pod/service CIDRs","If Azure stuck: Cancel in portal and delete managed resource to retry","Review AKS cluster size - smaller clusters create faster"],"related_patterns":["crossplane-managed-resource-synced-false","azure-quota-exceeded"]}
{"id":"controller-finalizer-stuck","category":"troubleshooting","symptoms":["Custom resource stuck in Terminating state","kubectl delete hangs indefinitely","Resource has finalizer but controller not removing it"],"root_cause":"Controller not properly removing finalizer during deletion, controller pod is down, or controller logic has bug in finalizer removal","investigation":["Check resource finalizers: kubectl get <resource> <name> -n <ns> -o yaml | grep finalizers","Verify controller pod is running: kubectl get pods -n <controller-ns>","Check controller logs for deletion errors: kubectl logs -n <controller-ns> <pod> | grep Deleting","Test if controller is processing deletions","Review controller code for removeFinalizer logic"],"fixes":["Restart controller pod if it's stuck: kubectl delete pod -n <controller-ns> <pod>","Manually remove finalizer: kubectl patch <resource> <name> -n <ns> -p '{\"metadata\":{\"finalizers\":[]}}' --type=merge","Review controller code - ensure finalizer removed before returning from reconcile","Check RBAC - controller needs update permission for finalizers","Add timeout to controller deletion logic to prevent stuck finalizers"],"related_patterns":["custom-crd-reconciliation-loop","crossplane-managed-resource-stuck-deleting"]}
{"id":"crossplane-package-install-failed","category":"troubleshooting","symptoms":["Provider or Configuration package shows Healthy=False","Package install stuck","ProviderRevision in Failed state"],"root_cause":"Package image pull failed, package has invalid YAML, or package dependencies not satisfied","investigation":["Check package status: kubectl get provider/configuration <name> -o yaml | grep -A10 status","Check revision status: kubectl get providerrevision/configurationrevision","Review package manager logs: kubectl logs -n crossplane-system -l app=crossplane","Test image pull: kubectl run test --image=<package-image> --rm -it","Check package dependencies in crossplane.yaml"],"fixes":["If image pull failed: Verify image exists and registry is accessible","Check package format: Download and validate YAML files in package","Resolve dependency conflicts: Update or remove conflicting packages","Force reinstall: Delete and recreate provider/configuration","Check Crossplane version compatibility with package"],"related_patterns":["image-pull-backoff","crossplane-provider-pod-crashloop"]}
