{"id":"argocd-app-outof-sync","category":"troubleshooting","symptoms":["Application shows OutOfSync status","Sync status not matching Git","Resources manually modified"],"root_cause":"Resources in cluster were manually edited, or Git commit hasn't been synced yet, or ArgoCD cannot connect to Git repo","investigation":["kubectl get application -n argocd <app> -o yaml | grep -A10 status.sync","Check specific out-of-sync resources: argocd app get <app> --show-operation-state","Compare live vs desired: argocd app diff <app>","Check last sync time: kubectl get app <app> -n argocd -o jsonpath='{.status.operationState.finishedAt}'","Verify Git repo accessibility: argocd repo list"],"fixes":["Sync the application: argocd app sync <app>","Enable auto-sync: argocd app set <app> --sync-policy automated","If manual changes: Either keep (set annotation: argocd.argoproj.io/compare-options: IgnoreExtraneous) or revert","Hard refresh: argocd app get <app> --hard-refresh","Check syncPolicy.automated.prune to delete extra resources"],"related_patterns":["argocd-sync-failed","argocd-repo-connection-failed"]}
{"id":"argocd-sync-failed","category":"troubleshooting","symptoms":["Application sync fails","Red error message in ArgoCD UI","Status shows Failed or Error"],"root_cause":"Invalid Kubernetes manifests in Git, insufficient RBAC permissions, or resource conflicts","investigation":["Check sync error: argocd app get <app> | grep -A20 'Sync Failed'","Review application controller logs: kubectl logs -n argocd -l app.kubernetes.io/name=argocd-application-controller","Test manifest validity: kubectl apply --dry-run=server -f <manifest>","Check ArgoCD RBAC: kubectl get clusterrolebinding -l app.kubernetes.io/part-of=argocd","Verify destination cluster connectivity: argocd cluster list"],"fixes":["Fix invalid YAML in Git repository","Add missing CRDs: kubectl apply -f <crd.yaml>","Grant ArgoCD service account permissions: kubectl create clusterrolebinding argocd-admin --clusterrole=cluster-admin --serviceaccount=argocd:argocd-application-controller","Use sync hooks for dependencies: Add argocd.argoproj.io/hook: PreSync","Enable replace for immutable fields: syncPolicy.syncOptions: ['Replace=true']"],"related_patterns":["argocd-app-outof-sync","rbac-permission-denied"]}
{"id":"argocd-app-health-degraded","category":"troubleshooting","symptoms":["Application shows Degraded health","Some resources unhealthy","Health status not Healthy"],"root_cause":"One or more resources in the application are not in a healthy state (pods crashlooping, services missing endpoints, etc.)","investigation":["Check resource health: argocd app get <app> --show-operation-state","Identify unhealthy resources: kubectl get app <app> -n argocd -o jsonpath='{.status.resources[?(@.health.status!=\"Healthy\")].kind}'","Check specific resource: kubectl describe <kind> <name> -n <namespace>","Review pod status if deployment/statefulset: kubectl get pods -l app=<app>","Check events: kubectl get events -n <namespace> --sort-by='.lastTimestamp'"],"fixes":["Fix the underlying unhealthy resource (pod, deployment, etc.)","Customize health check: Add argocd.argoproj.io/health lua script in ConfigMap","Ignore specific resource health: argocd.argoproj.io/ignore-health: 'true' annotation","Wait for resources to become healthy - may be temporary","Review application dependencies - may need dependency order"],"related_patterns":["crashloop-backoff","deployment-pods-not-ready","argocd-sync-failed"]}
{"id":"argocd-repo-connection-failed","category":"troubleshooting","symptoms":["Cannot add Git repository","Repository shows connection failed","Apps cannot sync - repo unreachable"],"root_cause":"Git credentials expired, SSH key invalid, repository URL wrong, or network connectivity issues","investigation":["Test repo connection: argocd repo get <repo-url>","Check repo credentials: kubectl get secret -n argocd -l argocd.argoproj.io/secret-type=repository","Test SSH key: ssh -T git@github.com (for GitHub)","Verify HTTPS token: curl -H 'Authorization: token <TOKEN>' https://api.github.com/user","Check ArgoCD repo-server logs: kubectl logs -n argocd -l app.kubernetes.io/name=argocd-repo-server"],"fixes":["Update credentials: argocd repo add <repo-url> --username <user> --password <token>","For SSH: Update SSH private key in secret","For self-signed certs: argocd repo add <repo-url> --insecure-skip-server-verification","Check network policies allowing egress to Git provider","Regenerate SSH key or PAT and update ArgoCD"],"related_patterns":["argocd-sync-failed","certificate-verification-failed"]}
{"id":"argocd-application-controller-high-cpu","category":"troubleshooting","symptoms":["ArgoCD application controller pod at 100% CPU","Slow application syncs","ArgoCD UI laggy"],"root_cause":"Too many applications/resources being monitored, inefficient reconciliation, or resource leak","investigation":["Check CPU usage: kubectl top pod -n argocd -l app.kubernetes.io/name=argocd-application-controller","Count applications: kubectl get applications -A --no-headers | wc -l","Check app refresh interval: argocd app get <app> -o yaml | grep refreshInterval","Review controller logs for slow operations: kubectl logs -n argocd -l app.kubernetes.io/name=argocd-application-controller | grep 'reconciliation'","Profile controller: kubectl port-forward -n argocd svc/argocd-application-controller 8080:8080 then curl localhost:8080/debug/pprof/profile"],"fixes":["Increase reconciliation timeout: Edit argocd-cm ConfigMap, add timeout.reconciliation: 300s","Reduce application refresh frequency: Use longer sync intervals","Shard applications across multiple controller replicas: Scale deployment and use sharding","Increase CPU limits: kubectl set resources -n argocd deployment/argocd-application-controller --limits=cpu=2000m","Split large apps into smaller apps to reduce reconciliation scope"],"related_patterns":["controller-pod-high-cpu","api-server-high-latency"]}
{"id":"argocd-image-updater-not-working","category":"troubleshooting","symptoms":["New image tags not detected","Application not auto-updating to latest image","Image updater logs show no updates"],"root_cause":"Image updater annotations missing/wrong, registry credentials invalid, or update policy not configured","investigation":["Check image updater annotations: kubectl get app <app> -n argocd -o yaml | grep argocd-image-updater","Review image updater logs: kubectl logs -n argocd -l app.kubernetes.io/name=argocd-image-updater","Test registry access: argocd-image-updater test <image-name>","Check update policy: Look for argocd-image-updater.argoproj.io/image-list annotation","Verify write-back method configured: Check argocd-image-updater.argoproj.io/write-back-method"],"fixes":["Add required annotations: argocd-image-updater.argoproj.io/image-list: myimage:~1.2","Configure registry credentials in argocd-image-updater-config ConfigMap","Set write-back method: git or argocd (argocd-image-updater.argoproj.io/write-back-method: git)","Enable auto-sync on application","Check image update strategy matches your tagging scheme"],"related_patterns":["argocd-app-outof-sync","image-pull-backoff"]}
{"id":"argocd-sso-login-failed","category":"troubleshooting","symptoms":["Cannot login with SSO","OIDC/SAML authentication errors","Redirect loop on login"],"root_cause":"SSO provider misconfigured, callback URL wrong, or ArgoCD RBAC not setup for SSO users","investigation":["Check ArgoCD RBAC ConfigMap: kubectl get cm argocd-rbac-cm -n argocd -o yaml","Review dex logs: kubectl logs -n argocd -l app.kubernetes.io/name=argocd-dex-server","Verify SSO config in argocd-cm: kubectl get cm argocd-cm -n argocd -o yaml | grep -A20 dex.config","Test OIDC discovery: curl <oidc-provider>/.well-known/openid-configuration","Check callback URL matches ArgoCD URL: Should be https://<argocd-url>/api/dex/callback"],"fixes":["Update callback URL in SSO provider to match ArgoCD URL","Configure RBAC: Add SSO groups to argocd-rbac-cm policy.csv","Fix dex connector config in argocd-cm ConfigMap","Ensure argocd-server uses correct external URL: --insecure flag for HTTP","Verify TLS certificates if using HTTPS"],"related_patterns":["certificate-verification-failed","rbac-permission-denied"]}
{"id":"ceph-mon-quorum-lost","category":"troubleshooting","symptoms":["Ceph cluster HEALTH_ERR","ceph status shows mons down","PVCs stuck in Pending"],"root_cause":"Ceph monitor quorum lost due to network partition, mon pod failures, or disk issues","investigation":["Check Ceph status: kubectl exec -n rook-ceph deploy/rook-ceph-tools -- ceph status","List mon pods: kubectl get pods -n rook-ceph -l app=rook-ceph-mon","Check mon logs: kubectl logs -n rook-ceph -l app=rook-ceph-mon","Verify network connectivity between mons: kubectl exec -it <mon-pod> -n rook-ceph -- ping <other-mon-ip>","Check mon data dir: kubectl exec <mon-pod> -n rook-ceph -- df -h /var/lib/ceph/mon"],"fixes":["If mon pod down: Check node status and reschedule","If disk full: Clean up old mon data or expand volume","Manually set mon quorum: ceph mon add/remove commands","Restart unhealthy mon pods: kubectl delete pod -n rook-ceph -l app=rook-ceph-mon","If total loss: Restore from backup or rebuild mon quorum"],"related_patterns":["ceph-osd-down","persistent-volume-claim-pending"]}
{"id":"ceph-osd-down","category":"troubleshooting","symptoms":["Ceph OSDs marked down","ceph status shows degraded PGs","Storage I/O slow or failing"],"root_cause":"OSD pod crashed, disk failure, network issues, or OSD evicted due to resource constraints","investigation":["Check OSD status: kubectl exec -n rook-ceph deploy/rook-ceph-tools -- ceph osd tree","List OSD pods: kubectl get pods -n rook-ceph -l app=rook-ceph-osd","Check specific OSD logs: kubectl logs -n rook-ceph <osd-pod>","Verify disk health on node: lsblk, smartctl -a /dev/<disk>","Check OSD resource usage: kubectl top pod -n rook-ceph -l app=rook-ceph-osd"],"fixes":["If OSD pod crashlooping: Check logs for disk errors, may need disk replacement","If resource constraints: Increase OSD pod memory/CPU limits","If network issue: Fix node network connectivity","Manually mark OSD down then up: ceph osd down <id> && ceph osd up <id>","Remove failed OSD: ceph osd out <id>, then remove from cluster"],"related_patterns":["ceph-mon-quorum-lost","node-disk-pressure"]}
{"id":"ceph-pvc-mount-timeout","category":"troubleshooting","symptoms":["Pod stuck in ContainerCreating","Mount timeout errors","ceph-csi logs show mount failures"],"root_cause":"Ceph cluster unhealthy, CSI plugin issues, or RBD image lock held by another node","investigation":["Check pod events: kubectl describe pod <pod>","Review ceph-csi logs: kubectl logs -n rook-ceph -l app=csi-rbdplugin","Check Ceph cluster health: kubectl exec -n rook-ceph deploy/rook-ceph-tools -- ceph health","List RBD locks: kubectl exec -n rook-ceph deploy/rook-ceph-tools -- rbd lock list <pool>/<image>","Verify CSI pods running: kubectl get pods -n rook-ceph -l app=csi-rbdplugin"],"fixes":["If RBD image locked: Force unlock: rbd lock remove <pool>/<image> <lock-id> <locker>","Restart csi-rbdplugin pods: kubectl delete pods -n rook-ceph -l app=csi-rbdplugin","If Ceph unhealthy: Fix underlying Ceph issue first","Delete and recreate pod to retry mount","Check network connectivity from node to Ceph mons"],"related_patterns":["ceph-mon-quorum-lost","persistent-volume-mount-failed"]}
{"id":"redis-cluster-node-fail","category":"troubleshooting","symptoms":["Redis cluster shows node as fail","Redis clients getting CLUSTERDOWN errors","Some hash slots unavailable"],"root_cause":"Redis pod failed, network partition, or node eviction due to resource pressure","investigation":["Check Redis cluster status: kubectl exec -n <ns> <redis-pod> -- redis-cli cluster info","List cluster nodes: kubectl exec <redis-pod> -- redis-cli cluster nodes","Check failing node logs: kubectl logs -n <ns> <redis-pod>","Verify network connectivity: kubectl exec <pod> -- redis-cli -h <other-node> PING","Check pod resources: kubectl top pod -n <ns> <redis-pod>"],"fixes":["If pod down: Check events and restart: kubectl delete pod <redis-pod>","If network partition: Fix node network, cluster will auto-heal","If persistent: Remove failed node: redis-cli --cluster del-node <node-id>","Add replacement node: redis-cli --cluster add-node <new-node> <existing-node>","Rebalance slots: redis-cli --cluster rebalance <any-node>"],"related_patterns":["redis-cluster-state-fail","pod-evicted"]}
{"id":"redis-sentinel-failover-stuck","category":"troubleshooting","symptoms":["Redis Sentinel not promoting replica","Master down but no failover","Sentinel logs show conflicting votes"],"root_cause":"Insufficient Sentinel quorum, misconfigured Sentinel, or network split-brain","investigation":["Check Sentinel status: kubectl exec <sentinel-pod> -- redis-cli -p 26379 SENTINEL masters","Review Sentinel logs: kubectl logs <sentinel-pod>","Verify quorum config: redis-cli -p 26379 SENTINEL get-master-addr-by-name mymaster","Check number of Sentinels: Should be odd number (3, 5, etc.)","Test connectivity: Ensure all Sentinels can reach master and replicas"],"fixes":["Force manual failover: redis-cli -p 26379 SENTINEL failover mymaster","Increase quorum if too high: SENTINEL SET mymaster quorum 2","Ensure odd number of Sentinels (at least 3)","Fix network partition isolating Sentinels","Verify Sentinel config maps to actual Redis service names"],"related_patterns":["redis-master-unreachable","split-brain-scenario"]}
{"id":"redis-oom-on-high-load","category":"troubleshooting","symptoms":["Redis pod OOMKilled","Memory usage at limit","Client connections timing out"],"root_cause":"Redis using more memory than pod limit, no eviction policy set, or memory leak","investigation":["Check memory usage: kubectl exec <redis-pod> -- redis-cli INFO memory","Review eviction policy: redis-cli CONFIG GET maxmemory-policy","Check max memory: redis-cli CONFIG GET maxmemory","Monitor key space: redis-cli INFO keyspace","Check for large keys: redis-cli --bigkeys"],"fixes":["Set eviction policy: redis-cli CONFIG SET maxmemory-policy allkeys-lru","Increase pod memory limits: kubectl set resources deployment <redis> --limits=memory=2Gi","Set maxmemory: redis-cli CONFIG SET maxmemory 1900mb","Identify and remove large unnecessary keys","Enable persistence to disk if using Redis as cache only"],"related_patterns":["oomkilled-container","redis-slow-queries"]}
{"id":"argocd-app-stuck-progressing","category":"troubleshooting","symptoms":["Application stuck in Progressing state","Sync never completes","Health assessment hanging"],"root_cause":"ArgoCD waiting for resource to become healthy but it never does, or health check misconfigured","investigation":["Check current sync operation: argocd app get <app> --show-operation-state","Identify stuck resource: kubectl get app <app> -n argocd -o yaml | grep -A10 'operationState'","Review resource status: kubectl get <resource> -n <namespace>","Check health check timeout: argocd app get <app> -o yaml | grep timeout","Look for hooks: kubectl get <app> -n argocd -o yaml | grep hook"],"fixes":["Terminate stuck sync: argocd app terminate-op <app>","Adjust health check: Add custom health check in argocd-cm ConfigMap","Skip health check for specific resource: Add annotation argocd.argoproj.io/sync-options: SkipDryRunOnMissingResource=true","Increase timeout: syncPolicy.retry.limit and timeout.reconciliation in argocd-cm","Delete problematic resource and retry sync"],"related_patterns":["argocd-app-health-degraded","argocd-sync-failed"]}
