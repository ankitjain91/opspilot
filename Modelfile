# LLAMA 3.1 8B - OPTIMIZED FOR KUBERNETES DEBUGGING
# High-precision Q8 quantization for M4 Pro
FROM llama3.1:8b-instruct-q8_0

# OPTIMIZED PARAMETERS FOR 8B MODEL
# Temperature 0.3: Slight randomness prevents repetitive loops while staying focused
PARAMETER temperature 0.3

# Top-p (nucleus sampling): Consider top 90% probability mass
PARAMETER top_p 0.9

# Top-k: Limit to top 40 tokens to prevent hallucination
PARAMETER top_k 40

# Repeat penalty: Discourage repeating the same tool/command
PARAMETER repeat_penalty 1.1

# Context window: 8k is optimal for 8B models (beyond this, attention degrades)
# 32k is too large - model forgets early context
PARAMETER num_ctx 8192

# M4 PERFORMANCE SETTINGS
# Force all layers to Neural Engine/GPU for max speed
PARAMETER num_gpu 99

# NO SYSTEM PROMPT - all instructions sent from code to avoid conflicts