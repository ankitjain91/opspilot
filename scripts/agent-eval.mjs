#!/usr/bin/env node
/**
 * Comprehensive Agent Evaluation Harness
 *
 * Tests the autonomous investigation agent's:
 * - Tool selection and chaining
 * - Hypothesis formation and tracking
 * - Confidence scoring
 * - Error recovery and alternatives
 * - Web search fallback
 * - MCP tool integration
 *
 * Run: node scripts/agent-eval.mjs
 */

const STEP_BUDGET = 10;
const CONFIDENCE_THRESHOLD = 60; // HIGH confidence

// =============================================================================
// MOCK IMPLEMENTATIONS (mirrors src/components/ai/types.ts logic)
// =============================================================================

/**
 * Mock confidence calculation (mirrors calculateConfidence from types.ts)
 *
 * CORE PRINCIPLE: Confirming a hypothesis IS the goal of investigation.
 * If we successfully identify the root cause, that's HIGH confidence.
 */
function calculateConfidence(state) {
    let score = 0;

    const successfulTools = state.toolHistory.filter(t => t.status === 'success' && t.useful).length;
    const evidenceSources = new Set(state.toolHistory.filter(t => t.useful).map(t => t.tool)).size;
    const confirmedHypotheses = state.hypotheses.filter(h => h.status === 'confirmed').length;
    const testedHypotheses = state.hypotheses.filter(h => h.status !== 'investigating').length;
    const errors = state.toolHistory.filter(t => t.status === 'error').length;

    // Hypothesis confirmation is the PRIMARY driver (max 40 points)
    // This is what matters most - did we find the answer?
    if (confirmedHypotheses > 0) {
        score += 35; // One confirmed hypothesis = major success
        score += Math.min((confirmedHypotheses - 1) * 5, 5); // Bonus for multiple
    } else if (testedHypotheses > 0) {
        score += 15; // Tested but refuted still shows progress
    } else if (state.hypotheses.length > 0) {
        score += 5; // At least we're investigating something
    }

    // Evidence contribution (max 30 points)
    score += Math.min(successfulTools * 8, 24);
    score += Math.min(evidenceSources * 3, 6);

    // Evidence quality bonus (max 15 points)
    const directEvidenceTools = ['GET_LOGS', 'GET_EVENTS', 'DESCRIBE'];
    const hasDirect = state.toolHistory.some(t =>
        t.useful && directEvidenceTools.includes(t.tool)
    );
    if (hasDirect) score += 15;
    else if (state.toolHistory.some(t => t.useful)) score += 10;
    else score += 3;

    // Playbook bonus (would be 5 points in real code)
    // Not simulated in test harness

    // Investigation thoroughness (max 5 points)
    score += Math.min(state.iteration || 1, 5);

    // Penalty for errors - minimal impact (max -5)
    // Errors are normal during investigation, shouldn't tank confidence
    score -= Math.min(errors, 5);

    score = Math.max(0, Math.min(100, score));
    const level = score >= 55 ? 'HIGH' : score >= 30 ? 'MEDIUM' : 'LOW';

    return { score, level };
}

/**
 * Mock hypothesis extraction (mirrors extractHypotheses from types.ts)
 */
function extractHypotheses(response, existing = []) {
    const hypotheses = [...existing];
    const now = Date.now();

    // Pattern 1: H1: cause format
    const hPattern = /[-‚Ä¢*]?\s*H(\d+)[:\s]+([^‚Üí\n]{10,150})(?:\s*‚Üí?\s*(?:Status:?\s*)?(\w+))?/gi;
    let match;
    while ((match = hPattern.exec(response)) !== null) {
        const id = `H${match[1]}`;
        const cause = match[2].trim();
        const statusText = (match[3] || 'investigating').toLowerCase();
        const status = statusText.includes('confirm') ? 'confirmed' :
                       statusText.includes('refut') ? 'refuted' : 'investigating';

        const existingIdx = hypotheses.findIndex(h => h.id === id);
        if (existingIdx >= 0) {
            hypotheses[existingIdx].status = status;
        } else if (cause.length >= 10) {
            hypotheses.push({ id, description: cause, status, evidence: [], createdAt: now });
        }
    }

    // Pattern 2: Natural language confirmation
    const confirmPatterns = [
        /root\s+cause[:\s]+(.{15,100}?)(?:\n|$)/gi,
        /confirmed?[:\s]+(.{15,100}?)(?:\n|$)/gi,
    ];
    for (const pattern of confirmPatterns) {
        pattern.lastIndex = 0;
        while ((match = pattern.exec(response)) !== null) {
            const cause = match[1].trim();
            // Mark matching hypotheses as confirmed
            for (const h of hypotheses) {
                if (h.status === 'investigating' &&
                    cause.toLowerCase().includes(h.description.toLowerCase().slice(0, 20))) {
                    h.status = 'confirmed';
                }
            }
        }
    }

    return hypotheses;
}

/**
 * Mock tool outcome evaluation
 */
function evaluateToolOutcome(result, toolName) {
    if (result.startsWith('‚ùå')) return { status: 'error', useful: false };
    if (result.startsWith('‚ö†Ô∏è')) return { status: 'partial', useful: result.length > 100 };
    if (!result || result.trim().length < 20) return { status: 'empty', useful: false };
    return { status: 'success', useful: true };
}

/**
 * Get next tool recommendations based on context
 */
function getNextToolRecommendations(state, lastResult) {
    const executedTools = new Set(state.toolHistory.map(t => t.tool));
    const recommendations = [];

    if (executedTools.size === 0) return ['FIND_ISSUES'];

    const resultLower = lastResult.toLowerCase();

    if (resultLower.includes('crashloop') || resultLower.includes('restart')) {
        if (!executedTools.has('GET_LOGS')) recommendations.push('GET_LOGS');
    }
    if (resultLower.includes('oom') || resultLower.includes('exit code 137')) {
        if (!executedTools.has('TOP_PODS')) recommendations.push('TOP_PODS');
    }
    if (resultLower.includes('pending') || resultLower.includes('scheduling')) {
        if (!executedTools.has('DESCRIBE')) recommendations.push('DESCRIBE');
    }
    if (recommendations.length === 0 && state.consecutiveUnproductive >= 1) {
        if (!executedTools.has('WEB_SEARCH')) recommendations.push('WEB_SEARCH');
    }

    return recommendations.slice(0, 3);
}

// =============================================================================
// TEST SCENARIOS
// =============================================================================

const scenarios = [
    {
        name: 'CrashLoop -> OOMKilled (Classic Investigation)',
        user: 'pod keeps crashing',
        expectedFlow: ['FIND_ISSUES', 'DESCRIBE', 'GET_LOGS'],
        expectedConfidence: 'HIGH',
        expectedRootCause: 'OOMKilled',
        toolOutputs: {
            'FIND_ISSUES:': `## Issues Found (3 total)
- [CRITICAL] Pod default/api-server-xyz: CrashLoopBackOff (Restarts: 47)
- [WARNING] Pod monitoring/prometheus-0: High restart count (12)
- [WARNING] Deployment default/api-server: 0/3 replicas ready`,
            'DESCRIBE:Pod default api-server-xyz': `## Pod: default/api-server-xyz
Status: CrashLoopBackOff
Restarts: 47
Last State: Terminated
Exit Code: 137
Reason: OOMKilled
Message: Container killed due to memory limit
Events:
- Warning BackOff: Back-off restarting container
- Normal Pulled: Successfully pulled image`,
            'GET_LOGS:default api-server-xyz': `## Logs: default/api-server-xyz
[INFO] Starting server...
[INFO] Loading config...
[WARN] High memory usage detected
[ERROR] std::bad_alloc: memory allocation failed
[FATAL] Out of memory - killing container
Killed`,
            'TOP_PODS:': `## Pod Resource Usage (10 pods)
| Namespace | Pod | CPU | Memory |
|-----------|-----|-----|--------|
| default | api-server-xyz | 450m | 510Mi |
| default | web-0 | 100m | 128Mi |`,
        },
        llmResponses: [
            {
                input: 'initial',
                output: `**Initial Hypotheses:**
- H1: Pod is crash-looping due to OOM (memory exhaustion) ‚Üí Status: INVESTIGATING
- H2: Application bug causing repeated crashes ‚Üí Status: INVESTIGATING

TOOL: FIND_ISSUES`
            },
            {
                input: 'after FIND_ISSUES',
                output: `Found api-server-xyz in CrashLoopBackOff with 47 restarts. Need to check exit code and logs.

- H1: OOM issue ‚Üí Status: INVESTIGATING (need exit code)

TOOL: DESCRIBE Pod default api-server-xyz
TOOL: GET_LOGS default api-server-xyz`
            },
            {
                input: 'after DESCRIBE and GET_LOGS',
                output: `## Summary
The pod api-server-xyz is being OOMKilled.

## Root Cause
**Exit code 137 (OOMKilled)** - the container is exceeding its memory limit of 512Mi.

Evidence:
- Exit Code: 137 (SIGKILL due to OOM)
- Logs show "std::bad_alloc" and "Out of memory"
- Memory usage at 510Mi hitting 512Mi limit

- H1: OOM issue ‚Üí Status: CONFIRMED

## Confidence: HIGH
Direct evidence from exit code, events, and logs all confirm OOM.

## Recommendation
Increase memory limit to 1-2Gi in the Deployment spec.`
            }
        ]
    },
    {
        name: 'Pending Pod -> Node Selector Mismatch',
        user: 'deployment not scaling, pods stuck',
        expectedFlow: ['FIND_ISSUES', 'DESCRIBE', 'GET_EVENTS'],
        expectedConfidence: 'HIGH',
        expectedRootCause: 'node selector',
        toolOutputs: {
            'FIND_ISSUES:': `## Issues Found (2 total)
- [CRITICAL] Pod payments/payment-processor-abc: Pending (scheduling failed)
- [WARNING] Deployment payments/payment-processor: 0/3 replicas ready`,
            'DESCRIBE:Pod payments payment-processor-abc': `## Pod: payments/payment-processor-abc
Status: Pending
Phase: Pending
Events:
- Warning FailedScheduling: 0/5 nodes are available: 5 node(s) didn't match Pod's node selector.
Node Selector:
  disk: ssd
  tier: premium`,
            'GET_EVENTS:payments': `## Recent Events (5)
- [Warning] payments/payment-processor-abc (Pod): FailedScheduling - 0/5 nodes available: 5 node(s) didn't match node selector
- [Normal] payments/payment-processor (Deployment): ScalingReplicaSet
- [Warning] payments/payment-processor-abc: FailedScheduling (repeated 15 times)`,
            'LIST_ALL:Node': `## Node List (5 total)
- node-1: Ready (disk=hdd, tier=standard)
- node-2: Ready (disk=hdd, tier=standard)
- node-3: Ready (disk=hdd, tier=standard)
- node-4: Ready (disk=ssd, tier=standard)
- node-5: Ready (disk=hdd, tier=premium)`,
        },
        llmResponses: [
            {
                input: 'initial',
                output: `**Initial Hypotheses:**
- H1: Resource constraints preventing scheduling ‚Üí Status: INVESTIGATING
- H2: Node selector or affinity mismatch ‚Üí Status: INVESTIGATING

TOOL: FIND_ISSUES`
            },
            {
                input: 'after FIND_ISSUES',
                output: `Found payment-processor pods in Pending state. Need to check scheduling events.

TOOL: DESCRIBE Pod payments payment-processor-abc
TOOL: GET_EVENTS payments`
            },
            {
                input: 'after DESCRIBE and GET_EVENTS',
                output: `## Summary
Payment processor pods cannot be scheduled due to node selector mismatch.

## Root Cause
**Node selector requires disk=ssd AND tier=premium, but no nodes match both labels.**

Evidence:
- Events show "5 node(s) didn't match Pod's node selector"
- Pod requires: disk=ssd, tier=premium
- Node-4 has disk=ssd but tier=standard
- Node-5 has tier=premium but disk=hdd

- H2: Node selector mismatch ‚Üí Status: CONFIRMED
- H1: Resource constraints ‚Üí Status: REFUTED

## Confidence: HIGH

## Recommendation
Either relax the node selector (remove tier requirement) or add a node with both labels.`
            }
        ]
    },
    {
        name: 'Service Connectivity Issue -> No Endpoints',
        user: 'service returning 503 errors',
        expectedFlow: ['FIND_ISSUES', 'GET_ENDPOINTS', 'DESCRIBE'],
        expectedConfidence: 'HIGH',
        expectedRootCause: 'no endpoints',
        toolOutputs: {
            'FIND_ISSUES:': `## Issues Found (2 total)
- [WARNING] Service frontend/web-svc: No endpoints
- [WARNING] Deployment frontend/web: 0/2 replicas ready`,
            'GET_ENDPOINTS:frontend web-svc': `## Endpoints: frontend/web-svc
**Ready:** None
**Not Ready:** None
**Ports:** 8080/TCP

‚ö†Ô∏è **No endpoints found!** This means no pods match the service selector.`,
            'DESCRIBE:Service frontend web-svc': `## Service: frontend/web-svc
Type: ClusterIP
Selector: app=web, version=v2
Ports: 8080/TCP
Endpoints: <none>`,
            'LIST_ALL:Pod': `## Pod List (5 total)
- frontend/web-abc: Running (app=web, version=v1)
- frontend/web-def: Running (app=web, version=v1)
- backend/api-123: Running`,
        },
        llmResponses: [
            {
                input: 'initial',
                output: `**Initial Hypotheses:**
- H1: Backend pods are unhealthy ‚Üí Status: INVESTIGATING
- H2: Service selector doesn't match pods ‚Üí Status: INVESTIGATING

TOOL: FIND_ISSUES`
            },
            {
                input: 'after FIND_ISSUES',
                output: `Found web-svc has no endpoints. Need to check service selector.

TOOL: GET_ENDPOINTS frontend web-svc
TOOL: DESCRIBE Service frontend web-svc`
            },
            {
                input: 'after GET_ENDPOINTS and DESCRIBE',
                output: `## Summary
The service has no endpoints because the selector doesn't match any pods.

## Root Cause
**Service selector mismatch** - service selects "app=web, version=v2" but pods have "version=v1".

- H2: Service selector mismatch ‚Üí Status: CONFIRMED

## Confidence: HIGH

## Recommendation
Update service selector to version=v1 or update pods to version=v2.`
            }
        ]
    },
    {
        name: 'Unknown Error -> Web Search Fallback',
        user: 'pod failing with OCI runtime error',
        expectedFlow: ['FIND_ISSUES', 'DESCRIBE', 'WEB_SEARCH'],
        expectedConfidence: 'HIGH',  // 4 useful tools + confirmed hypothesis = HIGH
        expectedRootCause: 'permission',  // Fix: the root cause is permission denied, not runtime
        toolOutputs: {
            'FIND_ISSUES:': `## Issues Found (1 total)
- [CRITICAL] Pod default/app-xyz: CreateContainerError`,
            'DESCRIBE:Pod default app-xyz': `## Pod: default/app-xyz
Status: CreateContainerError
Events:
- Warning Failed: Error response from daemon: OCI runtime create failed: container_linux.go:380: starting container process caused: exec: "/entrypoint.sh": permission denied: unknown`,
            'GET_LOGS:default app-xyz': `‚ùå Error: container not running`,
            'SEARCH_KNOWLEDGE:OCI runtime permission denied': `üìö No knowledge base articles found for "OCI runtime permission denied".`,
            'WEB_SEARCH:OCI runtime permission denied entrypoint': `## üåê Web Search Results for "kubernetes OCI runtime permission denied entrypoint"

### 1. Stack Overflow
**URL:** https://stackoverflow.com/questions/12345
The entrypoint script doesn't have execute permissions. Use chmod +x on the script or update the Dockerfile.

---

### 2. Kubernetes Documentation
**URL:** https://kubernetes.io/docs/concepts/workloads/pods/
Container runtime errors often occur when the entrypoint is not executable.`,
        },
        llmResponses: [
            {
                input: 'initial',
                output: `**Initial Hypotheses:**
- H1: Container image issue ‚Üí Status: INVESTIGATING
- H2: Permission problem with entrypoint ‚Üí Status: INVESTIGATING

TOOL: FIND_ISSUES`
            },
            {
                input: 'after FIND_ISSUES',
                output: `Found CreateContainerError. Need more details.

TOOL: DESCRIBE Pod default app-xyz`
            },
            {
                input: 'after DESCRIBE',
                output: `The error mentions "permission denied" for entrypoint.sh. Let me search for solutions.

TOOL: SEARCH_KNOWLEDGE OCI runtime permission denied`
            },
            {
                input: 'after SEARCH_KNOWLEDGE (no results)',
                output: `Knowledge base didn't have this. Searching the web.

TOOL: WEB_SEARCH OCI runtime permission denied entrypoint`
            },
            {
                input: 'after WEB_SEARCH',
                output: `## Summary
Container fails to start due to entrypoint script permission issue.

## Root Cause
The /entrypoint.sh script doesn't have execute permissions in the container image.

- H2: Permission problem ‚Üí Status: CONFIRMED

## Confidence: MEDIUM
Web search confirms the issue but direct fix requires image rebuild.

## Recommendation
Add "chmod +x /entrypoint.sh" to Dockerfile or use RUN chmod in a subsequent layer.`
            }
        ]
    },
    {
        name: 'Namespace Stuck Terminating',
        user: 'cannot delete namespace, stuck for hours',
        expectedFlow: ['GET_NAMESPACE', 'LIST_FINALIZERS'],
        expectedConfidence: 'HIGH',
        expectedRootCause: 'finalizer',
        toolOutputs: {
            'GET_NAMESPACE:old-project': `## Namespace: old-project
**Phase:** Terminating
**Deletion Requested:** 2024-01-15T10:00:00Z
**Finalizers:** kubernetes

### Conditions:
- **NamespaceDeletionDiscoveryFailure:** True - Discovery failed for some groups
- **NamespaceDeletionContentFailure:** True - Some content could not be deleted

### ‚ö†Ô∏è Namespace Stuck in Terminating
This namespace has a deletion timestamp but cannot be deleted.`,
            'LIST_FINALIZERS:old-project': `## Resources with Finalizers in old-project (2)

### CustomResource/database-backup üî¥ DELETING
**Finalizers:** database.example.com/cleanup
**Deletion Requested:** 2024-01-15T10:00:05Z
‚ö†Ô∏è This resource is stuck! The finalizer controller may be:
- Not running (check if the operator exists)
- Missing credentials

### PersistentVolumeClaim/data-pvc üî¥ DELETING
**Finalizers:** kubernetes.io/pvc-protection
**Deletion Requested:** 2024-01-15T10:00:10Z`,
            'FIND_ISSUES:': `## Issues Found (1 total)
- [WARNING] Namespace old-project: Terminating for 12h`,
        },
        llmResponses: [
            {
                input: 'initial',
                output: `**Initial Hypotheses:**
- H1: Resources with finalizers blocking deletion ‚Üí Status: INVESTIGATING
- H2: API server issues ‚Üí Status: INVESTIGATING

TOOL: GET_NAMESPACE old-project`
            },
            {
                input: 'after GET_NAMESPACE',
                output: `Namespace is terminating with conditions showing deletion failures. Need to find blocking resources.

TOOL: LIST_FINALIZERS old-project`
            },
            {
                input: 'after LIST_FINALIZERS',
                output: `## Summary
Namespace is stuck terminating due to resources with pending finalizers.

## Root Cause
**Custom resource database-backup has finalizer "database.example.com/cleanup" but the operator is not running to process it.**

- H1: Finalizers blocking deletion ‚Üí Status: CONFIRMED

## Confidence: HIGH
Direct evidence from LIST_FINALIZERS shows stuck resources.

## Recommendation
1. Check if database operator is running: kubectl get pods -A | grep database
2. If operator is gone, manually remove finalizer:
   kubectl patch customresource database-backup -n old-project -p '{"metadata":{"finalizers":[]}}' --type=merge`
            }
        ]
    },
    {
        name: 'Tool Failure Recovery',
        user: 'check pod nginx-broken',
        expectedFlow: ['DESCRIBE', 'LIST_ALL', 'DESCRIBE'],
        expectedConfidence: 'HIGH',  // Error, but 2 useful tools + 2 confirmed hypotheses = HIGH (68)
        expectedRootCause: 'image pull',
        toolOutputs: {
            'DESCRIBE:Pod default nginx-broken': `‚ùå Error: pods "nginx-broken" not found`,
            'LIST_ALL:Pod': `## Pod List (3 total)
- default/nginx-fixed-abc: ImagePullBackOff
- default/web-0: Running
- kube-system/coredns-xyz: Running`,
            'DESCRIBE:Pod default nginx-fixed-abc': `## Pod: default/nginx-fixed-abc
Status: ImagePullBackOff
Events:
- Warning Failed: Failed to pull image "nginx:latestt": rpc error: image not found
- Warning Failed: Error: ErrImagePull`,
        },
        llmResponses: [
            {
                input: 'initial',
                output: `**Initial Hypotheses:**
- H1: Pod name might be incorrect ‚Üí Status: INVESTIGATING
- H2: Image pull issue ‚Üí Status: INVESTIGATING

TOOL: DESCRIBE Pod default nginx-broken`
            },
            {
                input: 'after DESCRIBE (error)',
                output: `Pod not found with that exact name. Let me list all pods to find the correct name.

- H1: Pod name incorrect ‚Üí Status: INVESTIGATING

TOOL: LIST_ALL Pod`
            },
            {
                input: 'after LIST_ALL',
                output: `Found nginx-fixed-abc with ImagePullBackOff. This might be the pod user meant.

- H1: Pod name incorrect ‚Üí Status: CONFIRMED (found nginx-fixed-abc not nginx-broken)
- H2: Image pull issue ‚Üí Status: INVESTIGATING

TOOL: DESCRIBE Pod default nginx-fixed-abc`
            },
            {
                input: 'after second DESCRIBE',
                output: `## Summary
Pod nginx-fixed-abc cannot pull its image.

## Root Cause
**Image pull failure due to typo: "nginx:latestt" should be "nginx:latest"**

- H2: Image pull issue ‚Üí Status: CONFIRMED

## Confidence: MEDIUM
Found the issue but investigation had an error along the way.

## Recommendation
Fix the image tag in the deployment: kubectl set image deployment/nginx nginx=nginx:latest`
            }
        ]
    },
    // Edge case: RBAC/Permission issues
    {
        name: 'RBAC Permission Denied',
        user: 'cannot list pods in production namespace',
        expectedFlow: ['LIST_ALL', 'CLUSTER_HEALTH'],
        expectedConfidence: 'HIGH',  // Confirmed hypothesis = HIGH (58) - errors are normal
        expectedRootCause: 'RBAC',
        toolOutputs: {
            'LIST_ALL:Pod production': `‚ùå Error: pods is forbidden: User "dev-user" cannot list resource "pods" in API group "" in the namespace "production"`,
            'CLUSTER_HEALTH:': `## Cluster Health Summary
**Nodes:** 5/5 Ready
**Pods:** 245/250 Running
**Note:** Some namespaces may have RBAC restrictions.`,
            'RUN_KUBECTL:auth can-i list pods -n production': `no`,
        },
        llmResponses: [
            {
                input: 'initial',
                output: `**Initial Hypotheses:**
- H1: Pods don't exist in production ‚Üí Status: INVESTIGATING
- H2: RBAC permission issue ‚Üí Status: INVESTIGATING

TOOL: LIST_ALL Pod production`
            },
            {
                input: 'after LIST_ALL (forbidden)',
                output: `Got forbidden error. This is a permission issue, not a pod issue.

- H2: RBAC permission issue ‚Üí Status: INVESTIGATING

Let me check the cluster health to see if things are working in general.

TOOL: CLUSTER_HEALTH`
            },
            {
                input: 'after CLUSTER_HEALTH',
                output: `## Summary
User lacks permission to list pods in production namespace.

## Root Cause
**RBAC restriction** - User "dev-user" cannot list pods in "production" namespace.

- H1: Pods don't exist ‚Üí Status: REFUTED
- H2: RBAC permission issue ‚Üí Status: CONFIRMED

## Confidence: HIGH

## Recommendation
Contact cluster admin to grant ClusterRole/Role with 'list pods' permission, or use a service account with appropriate permissions.`
            }
        ]
    },
    // Edge case: Empty cluster / new deployment
    {
        name: 'Empty Namespace Investigation',
        user: 'why are there no pods running?',
        expectedFlow: ['FIND_ISSUES', 'LIST_ALL'],
        expectedConfidence: 'HIGH',  // 3 useful tools + confirmed hypothesis = HIGH (70)
        expectedRootCause: 'deployment',
        toolOutputs: {
            'FIND_ISSUES:': `## Issues Found (0 total)
No critical issues detected.

‚ö†Ô∏è The cluster appears healthy but some namespaces may be empty.`,
            'LIST_ALL:Pod': `## Pod List (0 in default namespace)
No pods found in the default namespace.

Hint: Check if deployments exist: LIST_ALL Deployment`,
            'LIST_ALL:Deployment': `## Deployment List (0 total)
No deployments found in the default namespace.`,
        },
        llmResponses: [
            {
                input: 'initial',
                output: `**Initial Hypotheses:**
- H1: Pods crashed and haven't restarted ‚Üí Status: INVESTIGATING
- H2: No deployments exist ‚Üí Status: INVESTIGATING

TOOL: FIND_ISSUES`
            },
            {
                input: 'after FIND_ISSUES',
                output: `No issues found. Let me check what pods exist.

TOOL: LIST_ALL Pod
TOOL: LIST_ALL Deployment`
            },
            {
                input: 'after LIST_ALL',
                output: `## Summary
No pods because no deployments exist in the default namespace.

## Root Cause
**No Deployment created** - The namespace has no workloads configured.

- H1: Pods crashed ‚Üí Status: REFUTED
- H2: No deployments exist ‚Üí Status: CONFIRMED

## Confidence: MEDIUM
This is expected state if no applications were deployed.

## Recommendation
Deploy your application: kubectl apply -f deployment.yaml`
            }
        ]
    },
    // Edge case: Multiple issues
    {
        name: 'Multiple Issues Triage',
        user: 'cluster is having issues',
        expectedFlow: ['FIND_ISSUES', 'DESCRIBE', 'GET_EVENTS'],
        expectedConfidence: 'HIGH',
        expectedRootCause: 'scheduling',
        toolOutputs: {
            'FIND_ISSUES:': `## Issues Found (5 total)
- [CRITICAL] Pod app/web-abc: CrashLoopBackOff (Restarts: 23)
- [CRITICAL] Pod app/worker-xyz: Pending (Unschedulable)
- [WARNING] Node node-3: DiskPressure
- [WARNING] Deployment app/web: 1/3 replicas ready
- [WARNING] Service app/api-svc: No endpoints`,
            'DESCRIBE:Pod app worker-xyz': `## Pod: app/worker-xyz
Status: Pending
Events:
- Warning FailedScheduling: 0/3 nodes available: 1 node has DiskPressure, 2 nodes have insufficient memory.`,
            'GET_EVENTS:app': `## Recent Events (10)
- [Warning] app/worker-xyz: FailedScheduling
- [Warning] app/web-abc: BackOff restarting container
- [Warning] app/web: MinimumReplicasUnavailable`,
            'GET_LOGS:app web-abc': `## Logs
[ERROR] Database connection refused
[FATAL] Cannot start without database`,
        },
        llmResponses: [
            {
                input: 'initial',
                output: `**Initial Hypotheses:**
- H1: Node resource pressure ‚Üí Status: INVESTIGATING
- H2: Application configuration issue ‚Üí Status: INVESTIGATING
- H3: Scheduling failure due to capacity ‚Üí Status: INVESTIGATING

TOOL: FIND_ISSUES`
            },
            {
                input: 'after FIND_ISSUES',
                output: `Found multiple issues. The Pending pod seems most critical - let's check why it's not scheduling.

- H3: Scheduling failure ‚Üí Status: INVESTIGATING

TOOL: DESCRIBE Pod app worker-xyz
TOOL: GET_EVENTS app`
            },
            {
                input: 'after DESCRIBE and GET_EVENTS',
                output: `## Summary
Multiple cascading issues in the app namespace.

## Root Cause
**Primary: Cluster capacity issue** - Node-3 has DiskPressure, and remaining nodes don't have enough memory.

This is causing:
1. worker-xyz cannot be scheduled
2. web pods are crashing (possibly due to missing worker)
3. api-svc has no endpoints (pods not ready)

- H1: Node resource pressure ‚Üí Status: CONFIRMED
- H3: Scheduling failure due to capacity ‚Üí Status: CONFIRMED

## Confidence: HIGH

## Recommendations
1. URGENT: Clear disk space on node-3 or add new node
2. Consider adding node with more memory
3. Once scheduling works, other issues may resolve`
            }
        ]
    },
];

// =============================================================================
// TEST RUNNER
// =============================================================================

function runScenario(scenario) {
    const state = {
        query: scenario.user,
        iteration: 0,
        maxIterations: STEP_BUDGET,
        toolHistory: [],
        hypotheses: [],
        consecutiveUnproductive: 0,
        phase: 'gathering',
    };

    const executedTools = new Set();
    const errors = [];
    let currentLlmResponseIdx = 0;
    let lastResult = '';

    console.log(`\n${'='.repeat(60)}`);
    console.log(`üìã Scenario: ${scenario.name}`);
    console.log(`   User Query: "${scenario.user}"`);
    console.log(`${'='.repeat(60)}`);

    // Simulate investigation loop
    while (state.iteration < state.maxIterations) {
        state.iteration++;

        // Get LLM response for current state
        const llmResponse = scenario.llmResponses[currentLlmResponseIdx];
        if (!llmResponse) {
            console.log(`   [Iteration ${state.iteration}] No more LLM responses defined`);
            break;
        }
        currentLlmResponseIdx++;

        console.log(`\n   [Iteration ${state.iteration}] LLM Input: ${llmResponse.input}`);

        // Extract hypotheses from LLM response
        state.hypotheses = extractHypotheses(llmResponse.output, state.hypotheses);
        if (state.hypotheses.length > 0) {
            console.log(`   üìä Hypotheses: ${state.hypotheses.map(h => `${h.id}(${h.status})`).join(', ')}`);
        }

        // Parse TOOL: commands from response
        const toolPattern = /TOOL:\s*(\w+)\s*(.*)?/gi;
        const tools = [...llmResponse.output.matchAll(toolPattern)];

        if (tools.length === 0) {
            // Check for final answer indicators
            if (llmResponse.output.includes('Confidence: HIGH') ||
                llmResponse.output.includes('Root Cause')) {
                console.log(`   ‚úÖ Final answer provided`);
                break;
            }
            state.consecutiveUnproductive++;
            console.log(`   ‚ö†Ô∏è No tools in response (unproductive: ${state.consecutiveUnproductive})`);
            continue;
        }

        // Execute tools
        for (const toolMatch of tools) {
            const toolName = toolMatch[1].toUpperCase();
            const toolArgs = (toolMatch[2] || '').trim();
            const key = `${toolName}:${toolArgs}`;

            if (executedTools.has(key)) {
                console.log(`   ‚è≠Ô∏è Skipping duplicate: ${key}`);
                continue;
            }
            executedTools.add(key);

            // Get mock output
            const output = scenario.toolOutputs[key];
            if (!output) {
                errors.push(`Missing mock output for: ${key}`);
                console.log(`   ‚ùå Missing mock: ${key}`);
                continue;
            }

            const outcome = evaluateToolOutcome(output, toolName);
            state.toolHistory.push({
                tool: toolName,
                args: toolArgs,
                result: output,
                ...outcome,
                timestamp: Date.now(),
            });

            lastResult = output;

            const statusEmoji = outcome.status === 'success' ? '‚úÖ' :
                               outcome.status === 'error' ? '‚ùå' : '‚ö†Ô∏è';
            console.log(`   ${statusEmoji} ${toolName}${toolArgs ? ` ${toolArgs}` : ''} ‚Üí ${outcome.status}${outcome.useful ? ' (useful)' : ''}`);

            // Reset unproductive counter on useful result
            if (outcome.useful) {
                state.consecutiveUnproductive = 0;
            } else {
                state.consecutiveUnproductive++;
            }
        }

        // Check recommendations
        const recommendations = getNextToolRecommendations(state, lastResult);
        if (recommendations.length > 0) {
            console.log(`   üí° Recommendations: ${recommendations.join(', ')}`);
        }
    }

    // Calculate final confidence
    const confidence = calculateConfidence(state);
    console.log(`\n   üìà Final Confidence: ${confidence.level} (${confidence.score}/100)`);

    // Check assertions
    const results = {
        scenario: scenario.name,
        passed: true,
        details: [],
    };

    // Check expected tools were used
    const usedTools = new Set(state.toolHistory.map(t => t.tool));
    for (const expectedTool of scenario.expectedFlow) {
        if (!usedTools.has(expectedTool)) {
            results.passed = false;
            results.details.push(`Missing tool: ${expectedTool}`);
        }
    }

    // Check confidence level
    if (confidence.level !== scenario.expectedConfidence) {
        results.passed = false;
        results.details.push(`Confidence mismatch: expected ${scenario.expectedConfidence}, got ${confidence.level}`);
    }

    // Check for confirmed hypothesis
    const confirmed = state.hypotheses.filter(h => h.status === 'confirmed');
    if (scenario.expectedConfidence === 'HIGH' && confirmed.length === 0) {
        results.passed = false;
        results.details.push('No hypothesis confirmed despite HIGH confidence expected');
    }

    // Check root cause detection (in final LLM response)
    const lastLlmResponse = scenario.llmResponses[scenario.llmResponses.length - 1];
    if (!lastLlmResponse.output.toLowerCase().includes(scenario.expectedRootCause.toLowerCase())) {
        results.passed = false;
        results.details.push(`Root cause "${scenario.expectedRootCause}" not found in final response`);
    }

    if (errors.length > 0) {
        results.passed = false;
        results.details.push(...errors);
    }

    // Print result
    console.log(`\n   ${'‚îÄ'.repeat(50)}`);
    if (results.passed) {
        console.log(`   ‚úÖ PASSED`);
    } else {
        console.log(`   ‚ùå FAILED`);
        for (const detail of results.details) {
            console.log(`      - ${detail}`);
        }
    }

    return results;
}

function main() {
    console.log(`
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë        AUTONOMOUS AGENT EVALUATION HARNESS                   ‚ïë
‚ïë        Testing investigation flow, tools, and confidence     ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
`);

    const results = [];
    let passed = 0;
    let failed = 0;

    for (const scenario of scenarios) {
        const result = runScenario(scenario);
        results.push(result);
        if (result.passed) passed++;
        else failed++;
    }

    // Summary
    console.log(`\n${'‚ïê'.repeat(60)}`);
    console.log(`SUMMARY: ${passed}/${scenarios.length} scenarios passed`);
    console.log(`${'‚ïê'.repeat(60)}`);

    for (const result of results) {
        const emoji = result.passed ? '‚úÖ' : '‚ùå';
        console.log(`${emoji} ${result.scenario}`);
    }

    if (failed > 0) {
        console.log(`\n‚ùå ${failed} scenario(s) failed`);
        process.exitCode = 1;
    } else {
        console.log(`\n‚úÖ All scenarios passed!`);
    }
}

main();
